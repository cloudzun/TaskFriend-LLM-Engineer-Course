{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c52147-0e28-4e4f-b4fe-2fdee0c32484",
   "metadata": {},
   "source": [
    "# æˆåŠŸçš„æç¤ºå·¥ç¨‹\n",
    "\n",
    "---\n",
    "\n",
    "åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»æ„å»ºäº†å…·å¤‡è®°å¿†åŠŸèƒ½çš„èŠå¤©æœºå™¨äººï¼Œå¹¶é€šè¿‡ RAG å°†å…¶è¿æ¥åˆ°çœŸå®ä¸–ç•Œçš„æ•°æ®ï¼Œè¿˜åˆ©ç”¨**ç³»ç»Ÿæç¤º**å°†å…¶è°ƒæ ¡ä¸ºä¸“ä¸šçš„ AI åŠ©æ‰‹ã€‚ç°åœ¨ï¼Œæ˜¯æ—¶å€™æ‹“å±•æˆ‘ä»¬å¯¹æç¤ºçš„è®¤çŸ¥äº†â€”â€”æç¤ºå¹¶ä¸ä»…ä»…æ˜¯ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æŸ¥è¯¢ã€‚åœ¨ä¸€å¥— LLM åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç»„åˆå¤šç§æç¤ºç±»å‹ï¼Œå¼•å¯¼æ¨¡å‹è¾“å‡ºç¬¦åˆé¢„æœŸçš„ç»“æœã€‚\n",
    "\n",
    "è™½ç„¶**ç³»ç»Ÿæç¤º**å®šä¹‰äº† LLM çš„äººæ ¼ï¼Œä½†ä»ç„¶å­˜åœ¨è®¸å¤šå…³é”®é—®é¢˜ç­‰å¾…è§£ç­”ï¼š\n",
    "\n",
    "* RAG å¢å¼ºçš„ LLM å¦‚ä½•ç†è§£å¹¶ä½¿ç”¨ä»çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Ÿ\n",
    "* å¦‚ä½•ç²¾è°ƒæç¤ºä»¥è·å¾—æœ€å¤§çš„æˆæ•ˆï¼Ÿ\n",
    "\n",
    "## æ•…äº‹è¿›å±•\n",
    "\n",
    "ç”¨æˆ·éå¸¸å–œæ¬¢ **TaskFriend**ï¼ä¸è¿‡ï¼Œæœ€è¿‘ä»–ä»¬å‘ç°è¾“å‡ºå­˜åœ¨ä¸ä¸€è‡´çš„é—®é¢˜ï¼š**TaskFriend** æœ‰æ—¶ä¼šè·‘é¢˜ï¼Œæˆ–é‡‡ç”¨ä¸é¢„æœŸä¸ç¬¦çš„æ ¼å¼ã€‚ç®€è€Œè¨€ä¹‹â€”â€”**TaskFriend** ç¼ºä¹æ˜ç¡®çš„å“åº”è§„èŒƒã€‚\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "* ç†è§£æç¤ºæ¨¡æ¿å¦‚ä½•å¼•å¯¼ LLM åœ¨ RAG ç³»ç»Ÿä¸­ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡\n",
    "* å­¦ä¼šè¿ç”¨ Role-Goal-Context-Audience-Format-Guardrails æ¡†æ¶è®¾è®¡é«˜æ•ˆçš„ç³»ç»Ÿæç¤º\n",
    "* æŒæ¡é«˜çº§æç¤ºå·¥ç¨‹æŠ€å·§ï¼šè§’è‰²/äººè®¾æç¤ºã€å°‘æ ·æœ¬å­¦ä¹ ã€é“¾å¼æ€ç»´\n",
    "* ä½¿ç”¨å…ƒæç¤ºï¼ˆMeta Promptingï¼‰è‡ªåŠ¨æ”¹è¿›æç¤º\n",
    "* åˆ©ç”¨â€œLLM ä½œä¸ºè£åˆ¤â€æŠ€æœ¯ï¼Œå®ç°è‡ªåŠ¨åŒ–è¯„ä¼°ä¸æ‰“åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d5ee3-f732-4996-ad08-7bdafbc6d8dc",
   "metadata": {},
   "source": [
    "## åˆå§‹åŒ–ç¯å¢ƒ\n",
    "\n",
    "### é…ç½® API Key\n",
    "\n",
    "åœ¨ä»»ä½•ç¬”è®°æœ¬å¼€å§‹å·¥ä½œå‰ï¼Œæˆ‘ä»¬éƒ½éœ€è¦åŠ è½½ [Model Studio çš„ API Key](https://modelstudio.console.alibabacloud.com/?tab=globalset#/efm/api_key)ï¼Œä»¥ä¾¿è°ƒç”¨æœ¬è¯¾ç¨‹ä¸­ä¼šæŒç»­ä½¿ç”¨çš„ Qwen æ¨¡å‹æ¥å£ã€‚\n",
    "\n",
    "> è‹¥ä¸æ¸…æ¥šå¦‚ä½•è·å– **Model Studio** API Keyï¼Œè¯·å‚è€ƒ `00 Setting Up the Environment` æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001b1c3-d5df-4bf9-89c0-c5d24b7f6e65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Model Studio API key\n",
    "import os\n",
    "from config.load_key import load_key\n",
    "load_key(\n",
    "    confirmation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652681b8-1d0d-47df-91e0-22d399dd29a7",
   "metadata": {},
   "source": [
    "### é…ç½® LLM ä¸åµŒå…¥æ¨¡å‹\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨é˜¿é‡Œäº‘çš„ `qwen-plus` ä½œä¸º LLMï¼Œå¹¶æ­é… DashScope çš„ `text-embedding-v3` ä½œä¸ºåµŒå…¥æ¨¡å‹ã€‚\n",
    "\n",
    "æœ¬ç« èŠ‚å°†æŠŠå‰é¢ä½¿ç”¨çš„ `OpenAI` å®¢æˆ·ç«¯æ›¿æ¢ä¸º `OpenAILike`ã€‚è¯¥å®¢æˆ·ç«¯æ˜¯ **LlamaIndex ä¸“ç”¨å°è£…**ï¼Œé€‚ç”¨äºä»¥ä¸‹å…¼å®¹ OpenAI åè®®çš„æ¨¡å‹ï¼š\n",
    "\n",
    "* Model Studio\n",
    "* DashScope\n",
    "* vLLM\n",
    "* Ollama\n",
    "* å…·å¤‡ OpenAI å…¼å®¹æ¥å£çš„æœ¬åœ° LLM\n",
    "\n",
    "> **æç¤ºï¼š** DashScope çš„ API ç«¯ç‚¹ä¸º `https://dashscope-intl.aliyuncs.com/api/v1`ï¼Œä¸åŒäºæˆ‘ä»¬æ­¤å‰ä½¿ç”¨çš„ `https://dashscope-intl.aliyuncs.com/compatible-mode/v1`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad59136-d04e-4de8-a712-4193183222ed",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set global settings\n",
    "import time\n",
    "import logging\n",
    "import dashscope\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from pathlib import Path\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Dashscope uses https://dashscope-intl.aliyuncs.com/api/v1 \n",
    "# instead of https://dashscope-intl.aliyuncs.com/compatible-mode/v1\n",
    "dashscope.base_http_api_url =\"https://dashscope-intl.aliyuncs.com/api/v1\"\n",
    "\n",
    "Settings.llm=OpenAILike(\n",
    "    model=\"qwen-plus\",\n",
    "    api_base=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    is_chat_model=True\n",
    ")\n",
    "\n",
    "Settings.embed_model = DashScopeEmbedding(\n",
    "    model_name=\"text-embedding-v3\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Global parameters set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe84a0-21f9-44fb-8010-d581ae8dc72e",
   "metadata": {},
   "source": [
    "### åŠ è½½ç´¢å¼•å¹¶å¯ç”¨ RAG\n",
    "\n",
    "ç”±äºæˆ‘ä»¬åœ¨å‰ä¸€ç« å·²ç»æ„å»ºå¹¶ä¿å­˜äº†ç´¢å¼•ï¼Œæ­¤å¤„åªéœ€ä»ä¿å­˜ä½ç½®ï¼ˆ`./knowledge_base/taskfriend`ï¼‰åŠ è½½å³å¯ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨ `llama_index` æä¾›çš„ä»¥ä¸‹ç»„ä»¶ï¼š\n",
    "\n",
    "* `StorageContext`ï¼šè´Ÿè´£å­˜å‚¨æ­¤å‰ä¿å­˜çš„èŠ‚ç‚¹ã€åµŒå…¥å‘é‡ä»¥åŠå‘é‡åº“ï¼Œä¾¿äºä»£ç è¿æ¥ç°æœ‰æ•°æ®ï¼Œä»è€Œæ— éœ€é‡æ–°å¤„ç†æ–‡æ¡£å³å¯æ¢å¤æ•´ä¸ªçŸ¥è¯†åº“ã€‚\n",
    "* `load_index_from_storage`ï¼šåŸºäº `StorageContext` ä¸­çš„æ–‡ä»¶é‡å»º RAG ç´¢å¼•ï¼Œæ˜¯å¿«é€Ÿæ¢å¤ RAG ç³»ç»Ÿçš„é«˜æ•ˆæ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501a0b1-97a4-4224-ae5c-7fca39bb8c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "persist_path=\"./knowledge_base/taskfriend\"\n",
    "\n",
    "# Import index (\"knowledgebase\") we built last chapter,\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=persist_path,\n",
    ")\n",
    "\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    embed_model=Settings.embed_model\n",
    ")\n",
    "print(f\"âœ… Index loaded from `{persist_path}`!\")\n",
    "\n",
    "# Build the query engine (used to implement RAG)\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True,\n",
    "    llm=Settings.llm,\n",
    ")\n",
    "print(\"âœ… Query engine built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70920303-7b64-441e-86d6-9d6170ce7535",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å·²åœ¨ä¸Šä¸€ç« é…ç½®äº† `get_qwen_stream_response` å‡½æ•°ï¼Œå› æ­¤æ­¤å¤„ç›´æ¥å¤ç”¨å³å¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb85d32-63c9-4cfb-a8f1-ed39f98a3047",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def update_prompt_template(\n",
    "        query_engine,\n",
    "        # Use LlamaIndex prompt template as default template\n",
    "        prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information and not prior knowledge, \"\n",
    "            \"answer the query.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "    )):\n",
    "    prompt_tmpl_str = prompt_tmpl_str\n",
    "    qa_prompt_tmpl = PromptTemplate(prompt_tmpl_str)\n",
    "    query_engine.update_prompts(\n",
    "        {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    "    )\n",
    "    return query_engine\n",
    "\n",
    "def ask_llm(query, query_engine):\n",
    "    streaming_response = query_engine.query(query)\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for token in streaming_response.response_gen:\n",
    "        print(token, end=\"\", flush=True)\n",
    "        full_response += token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd9b0e7-ab9f-4132-b29e-9cee7f9209cd",
   "metadata": {},
   "source": [
    "# æç¤ºåŸºç¡€ï¼šå¦‚ä½•æ„é€ ä¼˜è´¨æç¤º\n",
    "\n",
    "---\n",
    "\n",
    "## å¼ºåŠ›ç³»ç»Ÿæç¤ºçš„ç»„æˆï¼ˆå›é¡¾ï¼‰\n",
    "\n",
    "åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»äº†è§£åˆ°ï¼Œ**ç³»ç»Ÿæç¤º** å°±æ˜¯ AI çš„**å²—ä½è¯´æ˜ä¹¦**ã€‚ç¼ºä¹æ¸…æ™°æŒ‡ä»¤æ—¶ï¼Œæ¨¡å‹å¾€å¾€ä¼šè‡ªç”±å‘æŒ¥ï¼Œå¹¶äº§ç”Ÿä¸å°½å¦‚äººæ„çš„è¡¨ç°ã€‚\n",
    "\n",
    "æ­£å¦‚æ‚¨ä¸ä¼šå¯¹æ–°å…¥èŒçš„åŠ©ç†è¯´â€œéšä¾¿å’Œå›¢é˜Ÿæ‰“æˆä¸€ç‰‡â€é‚£æ ·ï¼Œæˆ‘ä»¬ä¹Ÿä¸åº”åªå¯¹ AI è¯´â€œè¯·å¸®å¿™â€ã€‚\n",
    "\n",
    "æ›´å¥½çš„åšæ³•æ˜¯æ˜ç¡®åœ°å®šä¹‰ï¼š\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "SP[System Prompt]\n",
    "SP --> R[Role <br><em>You are TaskFriend...</em>]\n",
    "SP --> G[Goal <br><em>Help users manage tasks</em>]\n",
    "SP --> C[Context<br><em>What information is available</em>]\n",
    "SP --> A[Audience<br><em>Users are professionals</em>]\n",
    "SP --> F[Format <br><em>Use bullet points</em>]\n",
    "SP --> GR[Guardrails <br><em>Never roleplay</em>]\n",
    "```\n",
    "\n",
    "åœ¨æ’°å†™**ç³»ç»Ÿæç¤º**æ—¶ï¼Œæ¸…æ™°ä¸ç²¾å‡†è‡³å…³é‡è¦ã€‚ä¸Šè¿°æ¡†æ¶ä¸ºè®¾è®¡ç³»ç»Ÿæç¤ºæä¾›äº†ä¸€ä¸ª**è“å›¾**ï¼Œå¸®åŠ©æˆ‘ä»¬æ„å»ºæ—¢å¯é åˆèšç„¦ä»»åŠ¡çš„æç¤ºã€‚\n",
    "\n",
    "| å…ƒç´  | ä½œç”¨ | ç¤ºä¾‹ |\n",
    "|--------|---------|---------|\n",
    "| **Role** | å®šä¹‰ AI çš„èº«ä»½ä¸äººè®¾ï¼Œå†³å®šäº¤äº’è¯­æ°”ã€‚ | `You are TaskFriend, a professional AI assistant for productivity and work-life balance.` |\n",
    "| **Goal** | æ˜ç¡®ä¼˜å…ˆå®Œæˆçš„ä»»åŠ¡ï¼Œç¡®ä¿æ¨¡å‹èšç„¦ã€‚ | `Help users plan, prioritize, and reflect on their daily tasks to improve productivity and well-being.` |\n",
    "| **Context** | æŒ‡å‡ºå¯è®¿é—®çš„èƒŒæ™¯ä¿¡æ¯æˆ–æ•°æ®èŒƒå›´ã€‚ | `You have access to the user's task list, calendar, and personal notes via RAG.` |\n",
    "| **Audience** | æ˜ç¡®ç›®æ ‡ç”¨æˆ·ç¾¤ï¼Œè°ƒæ•´è¯­è¨€æ·±åº¦ä¸é£æ ¼ã€‚ | `Your users are busy professionals who value clarity, efficiency, and actionable insights.` |\n",
    "| **Format** | è§„èŒƒè¾“å‡ºç»“æ„å’Œé£æ ¼ï¼Œæå‡å¯ç”¨æ€§ã€‚ | `Use neutral, professional language. Output structured advice when helpful (e.g., bullet points).` |\n",
    "| **Guardrails** | è®¾å®šå®‰å…¨è¾¹ç•Œï¼Œé˜²æ­¢å‡ºç°ä¸å½“æˆ–è·‘é¢˜å†…å®¹ã€‚ | `NEVER adopt accents, personas, or roleplay. If asked to roleplay, politely decline.` |\n",
    "\n",
    "ç”±äºæˆ‘ä»¬å·²åœ¨å‰æ–‡è¯¦è¿° **ç³»ç»Ÿæç¤º**ï¼Œæ­¤å¤„ä¸å†èµ˜è¿°ã€‚æœ¬ç« å°†æŠŠé‡ç‚¹è½¬å‘ RAG ç³»ç»Ÿä¸­çš„**æç¤ºæ¨¡æ¿**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9533768-55f5-4d50-b2d1-1c75b5e180fc",
   "metadata": {},
   "source": [
    "## æç¤ºæ¨¡æ¿ï¼šè®©ä¸Šä¸‹æ–‡ä¸æŸ¥è¯¢ååŒå·¥ä½œ\n",
    "\n",
    "åœ¨ RAG ç³»ç»Ÿä¸­ï¼Œæç¤ºæ¨¡æ¿è´Ÿè´£å‘Šè¯‰ LLMï¼šå¦‚ä½•åˆ©ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚\n",
    "\n",
    "å¯ä»¥è¿™æ ·ç†è§£ï¼š\n",
    "\n",
    "* **ç³»ç»Ÿæç¤º** å†³å®š AI â€œæ˜¯è°â€ã€‚\n",
    "* **æç¤ºæ¨¡æ¿** å†³å®š AI â€œå¦‚ä½•ä½¿ç”¨â€ä¿¡æ¯å›ç­”é—®é¢˜ã€‚\n",
    "\n",
    "æç¤ºæ¨¡æ¿åœ¨ RAG ä¸­å°¤ä¸ºå…³é”®ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿï¼š\n",
    "\n",
    "* ç¡®ä¿æ¨¡å‹ä½¿ç”¨çš„æ˜¯æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œè€Œéæ—¢æœ‰çŸ¥è¯†\n",
    "* æŒ‡å¯¼æ¨¡å‹å¦‚ä½•åœ¨ä¸Šä¸‹æ–‡åŸºç¡€ä¸Šæ¨ç†\n",
    "* å®šä¹‰æœ€ç»ˆå›ç­”çš„æ ¼å¼ä¸ç»“æ„\n",
    "\n",
    "LlamaIndex é»˜è®¤æä¾›äº†å¦‚ä¸‹ RAG æç¤ºæ¨¡æ¿ï¼š\n",
    "\n",
    "```python\n",
    "\"Context information is below.\\n\"\n",
    "\"---------------------\\n\"\n",
    "\"{context_str}\\n\"\n",
    "\"---------------------\\n\"\n",
    "\"Given the context information and not prior knowledge, \"\n",
    "\"answer the query.\\n\"\n",
    "\"Query: {query_str}\\n\"\n",
    "\"Answer: \"\n",
    "```\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "* `{context_str}`ï¼šä» RAG ç´¢å¼•æ£€ç´¢åˆ°çš„**ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼ˆå¦‚ä»»åŠ¡åˆ—è¡¨ã€æ—¥ç¨‹ä¿¡æ¯ç­‰ï¼‰\n",
    "* `{query_str}`ï¼šç”¨æˆ·çš„**è¾“å…¥é—®é¢˜**\n",
    "\n",
    "æ³¨æ„ï¼šè¿™ä¸æ˜¯ç³»ç»Ÿæç¤ºï¼Œè€Œæ˜¯æ¯æ¬¡æŸ¥è¯¢æ—¶ç”¨æ¥æŒ‡å¯¼ LLM è§£è¯»ä¸Šä¸‹æ–‡çš„æ¨¡æ¿ã€‚\n",
    "\n",
    "> æƒ³äº†è§£ LlamaIndex æä¾›çš„æ›´å¤šæç¤ºæ¨¡æ¿ï¼Œè¯·å‚è€ƒï¼š\n",
    "> * [Prompts](https://docs.llamaindex.ai/en/stable/module_guides/models/prompts/)ï¼ˆæç¤ºæ¨¡æ¿æ¦‚è§ˆï¼‰\n",
    "> * [default prompts](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/default_prompts.py)ï¼ˆé»˜è®¤æ¨¡æ¿æºç ï¼‰\n",
    "\n",
    "## ç³»ç»Ÿæç¤ºä¸æç¤ºæ¨¡æ¿çš„åŒºåˆ«\n",
    "\n",
    "| ç‰¹æ€§  | ç³»ç»Ÿæç¤º  | æç¤ºæ¨¡æ¿  |\n",
    "|----------|----------------|------------------|\n",
    "| ä½¿ç”¨æ—¶æœº | å¯¹è¯å¼€å§‹æ—¶ | æ¯æ¬¡é—®ç­”æ—¶ |\n",
    "| ä½œç”¨ | å®šä¹‰ AI çš„èº«ä»½ã€è¯­æ°”ä¸è¾¹ç•Œ | æŒ‡å¯¼æ¨¡å‹å¦‚ä½•ç»“åˆä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ |\n",
    "| ä½œç”¨èŒƒå›´ | è´¯ç©¿æ•´ä¸ªä¼šè¯ | é’ˆå¯¹æ¯æ¬¡æŸ¥è¯¢ |\n",
    "| ç¤ºä¾‹ | â€œYou are TaskFriend, a productivity assistant.â€ | â€œGiven the context below, answer the query.â€ |\n",
    "\n",
    "* **ç³»ç»Ÿæç¤º** å…³æ³¨ â€œAI æ˜¯è°â€\n",
    "* **æç¤ºæ¨¡æ¿** å…³æ³¨ â€œAI å¦‚ä½•å›ç­”â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2496380-7339-4597-80ee-015d68076644",
   "metadata": {},
   "source": [
    "# è¿›é˜¶ï¼šæ‰“é€ æ›´èªæ˜çš„æç¤ºæ¨¡æ¿\n",
    "\n",
    "---\n",
    "\n",
    "## æ¸…æ™°è¡¨è¾¾éœ€æ±‚\n",
    "\n",
    "LLM åœ¨é¢„è®­ç»ƒæ—¶æ¥è§¦è¿‡å¤§é‡æ ¼å¼åŒ–çš„æ–‡æœ¬ï¼Œè¿™å¯¹æˆ‘ä»¬æ¥è¯´æ˜¯å¥½æ¶ˆæ¯ï¼šæˆ‘ä»¬å¯ä»¥ç¼–å†™æ—¢æ–¹ä¾¿äººç±»é˜…è¯»ã€åˆèƒ½è®©æ¨¡å‹ç²¾å‡†ç†è§£çš„æç¤ºã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä¸€äº›æå‡æ¨¡æ¿å¯è¯»æ€§çš„æ’ç‰ˆå»ºè®®ï¼š\n",
    "\n",
    "* ä½¿ç”¨ `# æ ‡é¢˜` æ ‡æ³¨ç« èŠ‚ï¼š\n",
    "```markdown\n",
    "# Important Instructions\n",
    "```\n",
    "\n",
    "* ä½¿ç”¨åˆ†éš”çº¿ï¼ˆ`---`ï¼‰éš”å¼€ä¸Šä¸‹æ–‡ä¸æŸ¥è¯¢ï¼š\n",
    "```markdown\n",
    "-------------------------\n",
    "```\n",
    "\n",
    "* ä½¿ç”¨ \\<æ ‡ç­¾> æˆ– [æ‹¬å·] æ·»åŠ å…ƒæ•°æ®ï¼š\n",
    "```markdown\n",
    "[Context Source: Task List]\n",
    "<tag>Metadata</tag>\n",
    "```\n",
    "\n",
    "* ç”¨ **ç²—ä½“** æˆ– *æ–œä½“* çªå‡ºå…³é”®æŒ‡ä»¤ï¼š\n",
    "```markdown\n",
    "**Do not use prior knowledge.** Only use the *context above*.\n",
    "```\n",
    "\n",
    "è¿™äº›æ ¼å¼æœ‰åŠ©äºäººç±»ä¸æ¨¡å‹å‡†ç¡®ç†è§£æç¤ºç»“æ„ä¸æ„å›¾ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸º **TaskFriend** åˆ›å»ºä¸€ä¸ªç»“æ„åŒ–çš„æç¤ºæ¨¡æ¿ï¼Œä»¥æå‡è¾“å‡ºçš„æ¸…æ™°åº¦ä¸æ ¼å¼ç»Ÿä¸€æ€§ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f28314-01f5-4130-a9f1-f934c141afc0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = (\n",
    "    \"[Instructions] \\n\"\n",
    "    \"1. Do not make up tasks. \\n\"\n",
    "    \"2. You must emojis for each piece of information. \\n\"\n",
    "    \"3. Present information line-by-line. \\n\"\n",
    "    \"4. Only use information in the context to reply to user queries. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Query: {query_str}\\nã€‚\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "update_prompt_template(query_engine,prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69507dc7-3499-496b-b829-1637e67a2a49",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    Give me a random task from my task list.\n",
    "\"\"\"\n",
    "ask_llm(query,query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e33410-e443-4d8d-ae60-c7b4217e31ef",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°ï¼Œ**TaskFriend** èƒ½ä¸¥æ ¼éµå¾ªæˆ‘ä»¬è®¾å®šçš„æ ¼å¼è¦æ±‚ã€‚**æç¤ºæ¨¡æ¿** çš„å¼ºå¤§ä¹‹å¤„åœ¨äºï¼Œå®ƒèƒ½åœ¨ RAG ç³»ç»Ÿä¸­æ‰¿æ‹…å¤šç§èŒè´£ï¼Œçµæ´»å¼•å¯¼è¾“å‡ºã€‚\n",
    "\n",
    "## å±€é™æ€§\n",
    "\n",
    "ä¸è¿‡ä½ ä¼šæ³¨æ„åˆ°ï¼Œæç¤ºæ¨¡æ¿å¯¹äºä¸ä¸Šä¸‹æ–‡æ— å…³çš„æŒ‡ä»¤å¹¶ä¸ååˆ†â€œå¼ºåŠ¿â€ã€‚\n",
    "\n",
    "è¯•è¯•çœ‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2fc2d5-c4bb-45f9-9aba-66c0d6c2acfb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    Give me a random task from my task list.\n",
    "    Do not use emojis.\n",
    "\"\"\"\n",
    "ask_llm(query,query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8019b-8f0b-4a9f-869b-bc668c93e3ec",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    Give me a task not from my task list.\n",
    "    Do not use emojis.\n",
    "\"\"\"\n",
    "ask_llm(query,query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a67310-157f-456e-ab9c-19b33b1b9eb8",
   "metadata": {},
   "source": [
    "ç”¨æˆ·åœ¨æŸ¥è¯¢ä¸­å†™ä¸‹çš„ä»»ä½•æŒ‡ä»¤ï¼Œæ¨¡å‹éƒ½ä¼šå°½é‡éµä»â€”â€”å“ªæ€•ä¼šè¦†ç›–æˆ‘ä»¬åœ¨æ¨¡æ¿ä¸­è®¾å®šçš„éƒ¨åˆ†è§„åˆ™ã€‚\n",
    "\n",
    "ä¸è¿‡ï¼Œåªè¦æ¶‰åŠä¸Šä¸‹æ–‡å†…å®¹ï¼Œä½ ä¼šå‘ç°æç¤ºæ¨¡æ¿ä»èƒ½å‘æŒ¥ä½œç”¨ã€‚\n",
    "\n",
    "## ä¸ºä»€ä¹ˆä»è¦ä½¿ç”¨æç¤ºæ¨¡æ¿ï¼Ÿ\n",
    "\n",
    "### å®ƒä»¬æ˜¯å¼€å‘è€…å·¥å…·ï¼Œè€Œéé˜²ç«å¢™\n",
    "\n",
    "æç¤ºæ¨¡æ¿çš„ç›®æ ‡å¹¶éé˜»æ­¢ç”¨æˆ·æˆ–å¼ºåˆ¶æ‰§è¡Œè¡Œä¸ºï¼Œè€Œæ˜¯ä¸ºäº†ï¼š\n",
    "\n",
    "* æ ‡å‡†åŒ–ä¼ é€’ç»™ LLM çš„è¾“å…¥\n",
    "* æå‡è¾“å‡ºçš„ä¸€è‡´æ€§\n",
    "* è®©æç¤ºå·¥ç¨‹æ›´æ˜“å¤ç”¨ä¸æµ‹è¯•\n",
    "\n",
    "### å®ƒä»¬èƒ½æå‡ RAG å›ç­”è´¨é‡\n",
    "\n",
    "åœ¨ RAG ç³»ç»Ÿä¸­ï¼Œæç¤ºæ¨¡æ¿æ˜¯æ•™ä¼šæ¨¡å‹â€œå¦‚ä½•ä½¿ç”¨ä¸Šä¸‹æ–‡â€çš„å…³é”®ã€‚\n",
    "\n",
    "è‹¥æ²¡æœ‰ä¼˜ç§€çš„æ¨¡æ¿ï¼Œæ¨¡å‹å¯èƒ½ä¼šï¼š\n",
    "\n",
    "* å¿½è§†ä¸Šä¸‹æ–‡\n",
    "* é€€å›åˆ°è‡ªèº«è®­ç»ƒçŸ¥è¯†\n",
    "* ç»™å‡ºå«ç³Šæˆ–æ³›æ³›çš„ç­”æ¡ˆ\n",
    "\n",
    "è€Œè®¾è®¡è‰¯å¥½çš„æ¨¡æ¿èƒ½å¤Ÿç¡®ä¿æ¨¡å‹ç¡®å®åˆ©ç”¨äº†æ£€ç´¢åˆ°çš„çŸ¥è¯†ã€‚\n",
    "\n",
    "### å®ƒä»¬èƒ½å¼ºåŒ–ç»“æ„åŒ–è¾“å‡º\n",
    "\n",
    "é€šè¿‡æç¤ºæ¨¡æ¿ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å¯¼æ¨¡å‹è¾“å‡ºï¼š\n",
    "\n",
    "* JSON\n",
    "* è¦ç‚¹åˆ—è¡¨\n",
    "* è¡¨æ ¼\n",
    "* ä»£ç \n",
    "* Markdown\n",
    "\n",
    "è¿™æ ·æœ‰åŠ©äºä¸‹æ¸¸åº”ç”¨è§£æä¸è°ƒåº¦æ¨¡å‹è¾“å‡ºã€‚æ›´æ£’çš„æ˜¯ï¼Œè¿™ç±»ç»“æ„åŒ–çº¦æŸåœ¨æç¤ºæ¨¡æ¿ä¸­éå¸¸å¯é ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511989af-4001-4b74-ac91-443c231468d1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = (\n",
    "    \"[Instructions] \\n\"\n",
    "    \"1. Do not make up tasks. \\n\"\n",
    "    \"2. Only use information in the context to reply to user queries. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"[Output format] \\n\"\n",
    "    \"Answer in JSON format with the following structure:\\n\"\n",
    "    \"{{\\n\"\n",
    "    \"  'task': string,\\n\"\n",
    "    \"  'due_date': string,\\n\"\n",
    "    \"  'priority': string\\n\"\n",
    "    \"}}\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "update_prompt_template(query_engine,prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229ad25-6e70-41c7-bdad-9644faa4f119",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    Give me a task from my task list.\n",
    "    Format each item as a bullet point.\n",
    "\"\"\"\n",
    "ask_llm(query,query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089f213-5d18-408c-a7c6-d3938dddba40",
   "metadata": {},
   "source": [
    "# é€šç”¨æç¤ºå·¥ç¨‹æŠ€å·§ï¼šä¼˜åŒ– AI è¡Œä¸º\n",
    "\n",
    "---\n",
    "\n",
    "å‰æ–‡æˆ‘ä»¬å·²ç»è®¨è®ºäº† **ç³»ç»Ÿæç¤º** ä¸ **æç¤ºæ¨¡æ¿**ã€‚æ¥ä¸‹æ¥ï¼Œå°†æŠŠç„¦ç‚¹æ”¾åœ¨â€œæç¤ºæœ¬èº«èƒ½åšåˆ°ä»€ä¹ˆâ€ä»¥åŠâ€œå¦‚ä½•è¿­ä»£æç¤ºä»¥æŒç»­æ”¹è¿›æ•ˆæœâ€ã€‚æŒæ¡è¿™äº›æŠ€å·§åï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å…¶æ•´åˆè¿›ç”¨æˆ·ä¸å¯è§çš„ç³»ç»Ÿæç¤ºä¸æ¨¡æ¿ä¹‹ä¸­ã€‚\n",
    "\n",
    "> **æç¤ºï¼š** ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œæœ¬èŠ‚å°†ä»¥â€œç”¨æˆ·æŸ¥è¯¢â€çš„æ–¹å¼ç¼–å†™æç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥ä¿®æ”¹ç³»ç»Ÿæç¤ºæˆ–æ¨¡æ¿ï¼Œä»è€Œå‡å°‘æ‰€éœ€çš„ä»£ç æ”¹åŠ¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182826d-a764-496e-a52b-a5a19fb82a8c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "def get_qwen_stream_response(query):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-plus\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        stream=True\n",
    "    )\n",
    "    full_response = \"\"\n",
    "    \n",
    "    for chunk in completion:\n",
    "        try:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_response += content\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        time.sleep(0)\n",
    "    \n",
    "    print()\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d3ec0-0e76-4c16-8ac1-bb8e1b9bf053",
   "metadata": {},
   "source": [
    "## è§’è‰²ä¸äººè®¾\n",
    "\n",
    "æ‚¨çš„ LLM åº”ç”¨ç†åº”å…·å¤‡ç‹¬ç‰¹çš„é£æ ¼ï¼Œå®ƒçš„è¯´è¯æ–¹å¼ä¸å¤„ç†ç»“æœéƒ½åº”è¯¥ç¬¦åˆç‰¹å®šåœºæ™¯éœ€æ±‚ã€‚\n",
    "\n",
    "åœ¨äº§å“è®¾è®¡ä¸­ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ï¼šåº”ç”¨å®šä½ä¼šç›´æ¥å½±å“ç”¨æˆ·æ„ŸçŸ¥ã€‚\n",
    "\n",
    "æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•å®éªŒæ¥æ„Ÿå—å·®å¼‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af5b4f-2f15-46c9-b7fc-b32ed54051da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the question\n",
    "question = \"\"\"\n",
    "    What is Qwen?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebc774-6852-40a4-862a-754f39983955",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let LLM take on the role of a professional LLM developer\n",
    "pseudo_template = \"\"\"\n",
    "    You are a professional LLM developer.\n",
    "    Your job is to explain LLM technology and concepts in-depth.\n",
    "    Limit your explaination to 100 words.\n",
    "\"\"\"\n",
    "\n",
    "query = pseudo_template + question\n",
    "\n",
    "response = get_qwen_stream_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ddeae7-f6c3-4993-b4b3-1dc9ddf9fb21",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let LLM take on the role of a kindergarden teacher\n",
    "pseudo_template = \"\"\"\n",
    "    You are a kindergarden teacher.\n",
    "    Your job is to explain complex ideas in easy-to-understand comparisons.\n",
    "    Explain like your audience is 5 years old.\n",
    "    Limit your explaination to 100 words.\n",
    "\"\"\"\n",
    "\n",
    "query = pseudo_template + question\n",
    "\n",
    "response = get_qwen_stream_response(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdd457-5b7c-4690-85e1-c5024b782025",
   "metadata": {},
   "source": [
    "é¢„æœŸè¾“å‡ºï¼š\n",
    "\n",
    "| è§’è‰²/äººè®¾  | LLM å¼€å‘è€…  | å¹¼å„¿å›­è€å¸ˆ  |\n",
    "|-------|----------------|------------------------|\n",
    "| æç¤º | You are a professional LLM developer.<br>Your job is to explain LLM technology and concepts in-depth.<br>Limit your explaination to 100 words.  | You are a kindergarden teacher.<br>Your job is to explain complex ideas in easy-to-understand comparisons.<br>Explain like your audience is 5 years old.<br>Limit your explaination to 100 words.  |\n",
    "| æŸ¥è¯¢ | What is Qwen?  | What is Qwen?  |\n",
    "| è¾“å‡º  | Qwen is a large language model (LLM) developed by Alibaba Cloud, designed to understand and generate human-like text across multiple languages and domains. It leverages deep learning techniques, particularly transformer architectures, to process input and produce coherent, contextually relevant responses. Qwen supports various tasks such as translation, question-answering, content creation, and code generation, making it versatile for both research and practical applications.  | Qwen is like a super-smart robot friend who lives inside a computer. Just like how you ask your teacher questions, people can ask Qwen all sorts of questions, and it tries to give helpful answers! It's like having a magic helper that knows lots of things and can talk to you through typing. But remember, it's not a real person - it's just really good at pretending to be one when it helps you learn or solve problems!  |\n",
    "\n",
    "å¯ä»¥çœ‹åˆ°ï¼Œåªéœ€ç®€å•çš„æç¤ºå·¥ç¨‹ï¼Œå°±èƒ½è®© LLM ä»¥æŒ‡å®šè§’è‰²æˆ–äººè®¾çš„æ–¹å¼â€œè¯´è¯â€ã€‚è¿™ä¸ºæˆ‘ä»¬è®¾å®šåº”ç”¨è°ƒæ€§å¥ å®šäº†åŸºç¡€ã€‚\n",
    "\n",
    "## å•æ ·æœ¬ä¸å°‘æ ·æœ¬å­¦ä¹ \n",
    "\n",
    "æˆ‘ä»¬è¿˜å¯ä»¥æŠŠ One-shot / Few-shot ç¤ºä¾‹ç›´æ¥å†™å…¥æç¤ºä¸­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de382f-d26e-4a70-be4c-dbcbe062941d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the question\n",
    "question = \"\"\"\n",
    "    Recipe for Old Fashioned.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43710d-b2df-4e6b-99d3-7148fde30c54",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let the LLM work out on its own\n",
    "pseudo_template = \"\"\"\n",
    "    [Goal]\n",
    "    Create content for user based on input\n",
    "    ---------\n",
    "    [Output format]\n",
    "    1. Markdown format\n",
    "    2. Use properly formatted lists and tables\n",
    "    ---------\n",
    "    [User input]\n",
    "    The following requirement is provided by the user:\n",
    "\"\"\"\n",
    "\n",
    "query = pseudo_template + question\n",
    "\n",
    "response = get_qwen_stream_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7e306-695f-4c4e-967f-76868ebbb7f1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provide one-and-few-shot example\n",
    "pseudo_template = \"\"\"\n",
    "    [Goal]\n",
    "    Create content for user based on input\n",
    "    ---------\n",
    "    [Output format]\n",
    "    1. Markdown format\n",
    "    2. Use properly formatted lists and tables\n",
    "    ---------\n",
    "    [Example]\n",
    "    # Recipe name\n",
    "    Short description of recipe\n",
    "    \n",
    "    ## Ingredients\n",
    "    1. x tbsp - Ingredient 1\n",
    "    2. y tsp - Ingredient 2\n",
    "    \n",
    "    ## Instructions\n",
    "    1. Do X\n",
    "    2. Do Y\n",
    "    \n",
    "    ## Tips\n",
    "    ---\n",
    "    [User input]\n",
    "    The following requirement is provided by the user:\n",
    "\"\"\"\n",
    "\n",
    "query = pseudo_template + question\n",
    "\n",
    "response = get_qwen_stream_response(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92517064-c30a-4a19-b5c3-640ac2ea47c5",
   "metadata": {},
   "source": [
    "è¿™å¾ˆç›´è§‚â€”â€”ç¤ºä¾‹ä¼šå¼•å¯¼æ¨¡å‹æ§åˆ¶å†…å®¹çš„èŒƒå›´ä¸è¾“å‡ºæ ¼å¼ã€‚\n",
    "\n",
    "## é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰\n",
    "\n",
    "æˆ‘ä»¬åŒæ ·å¯ä»¥é€šè¿‡æç¤ºè®© LLM æ˜¾å¼ï¼ˆæˆ–éšå¼ï¼‰æ‰§è¡Œé“¾å¼æ€ç»´ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020d3ea-3043-433b-a99a-afa55c692c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "    A large jar contains a mix of coins: only pennies (1Â¢), nickels (5Â¢), and dimes (10Â¢).\n",
    "    \n",
    "    There are:\n",
    "    * 100 coins in total,\n",
    "    * The total value of the coins is exactly $5.00 (that is, 500 cents),\n",
    "    * The number of pennies is equal to the number of nickels and dimes combined.\n",
    "    \n",
    "    How many of each type of coin are in the jar?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed2a71-6f8d-488e-8b97-dcee39a6e08f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pseudo_template = \"\"\"\n",
    "    Help the user solve math problems\n",
    "    Give the answer directly, DO NOT show your work\n",
    "\"\"\"\n",
    "\n",
    "query = pseudo_template + question\n",
    "\n",
    "response = get_qwen_stream_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780cf0fa-71f9-493b-9138-9bf808fc466c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pseudo_template = \"\"\"\n",
    "    Help the user solve math problems\n",
    "    Show your work step-by-step, grouped by logical sections\n",
    "\"\"\"\n",
    "\n",
    "query = pseudo_template + question\n",
    "\n",
    "response = get_qwen_stream_response(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b2f78-26dd-4669-991f-31082fd2a590",
   "metadata": {},
   "source": [
    "è¿™æ˜¯ä¸ªé¢‡æœ‰æ„æ€çš„å®éªŒï¼š\n",
    "\n",
    "* ä¸å±•ç¤ºæ¨ç†è¿‡ç¨‹æ—¶ï¼Œæ¨¡å‹å¯èƒ½ç»™å‡º**é”™è¯¯**ç­”æ¡ˆ\n",
    "* å±•ç¤ºæ¨ç†è¿‡ç¨‹æ—¶ï¼Œæ¨¡å‹å¾€å¾€èƒ½å¾—å‡º**æ­£ç¡®**ç­”æ¡ˆ\n",
    "\n",
    "Chain-of-Thoughtï¼ˆåˆç§° **Reasoning**ï¼‰èƒ½å¤Ÿå¸®åŠ©æ¨¡å‹â€œè‡ªæˆ‘æ€è€ƒâ€ï¼Œæå‡ç­”æ¡ˆè´¨é‡ã€‚\n",
    "\n",
    "## å½©è›‹ï¼šæ¨ç†æ¨¡å‹\n",
    "\n",
    "å»¶ç»­ä¸Šä¸€èŠ‚å†…å®¹ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚è¯¸å¦‚ Qwen3ï¼ˆæ€è€ƒæ¨¡å¼ï¼‰ã€åŸºäº Qwen2.5 çš„ QwQã€DeepSeek-R1 ç­‰æ¨¡å‹ï¼Œéƒ½å¯ä»¥åœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆå‰è¾“å‡ºå®Œæ•´çš„æ¨ç†è¿‡ç¨‹ã€‚\n",
    "\n",
    "> æƒ³äº†è§£ Model Studio ä¸Šçš„æ¨ç†æ¨¡å‹ï¼Œè¯·å‚é˜… [Deep Thinking](https://www.alibabacloud.com/help/en/model-studio/deep-thinking)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902975d-74d9-4723-9ca6-7ffb38a3ca80",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create reasoning function\n",
    "def get_qwen_reasoning(query, verbose=True):\n",
    "    \n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-plus-2025-04-28\",                       # â† Use the \"qwen-plus-2025-04-28\" model\n",
    "        messages = [{\"role\": \"user\", \"content\": query}],\n",
    "        extra_body={\n",
    "            \"enable_thinking\": True,                        # â† Enable thinking\n",
    "            # \"thinking_budget\": 200                          # â† Enable to limit tokens used for thinking\n",
    "        },\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    reasoning_content = \"\"\n",
    "    answer_content = \"\"\n",
    "    is_answering = False\n",
    "    print(\"\\n\" + \"=\" * 20 + \"Thinking Process\" + \"=\" * 20 + \"\\n\")\n",
    "\n",
    "    for chunk in completion:\n",
    "        if not chunk.choices:\n",
    "            continue\n",
    "\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        # Capture reasoning content\n",
    "        if hasattr(delta, \"reasoning_content\") and delta.reasoning_content is not None:\n",
    "            if verbose:\n",
    "                print(delta.reasoning_content, end=\"\", flush=True)\n",
    "            reasoning_content += delta.reasoning_content\n",
    "\n",
    "        # Capture final answer\n",
    "        if hasattr(delta, \"content\") and delta.content:\n",
    "            if not is_answering and verbose:\n",
    "                print(\"\\n\" + \"=\" * 20 + \"Complete Response\" + \"=\" * 20 + \"\\n\")\n",
    "                is_answering = True\n",
    "            if verbose:\n",
    "                print(delta.content, end=\"\", flush=True)\n",
    "            answer_content += delta.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e56542-9bb7-43a7-aec5-1846e85ebfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    Why is the sky blue?\n",
    "\"\"\"\n",
    "\n",
    "get_qwen_reasoning(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea92a3e-d6b0-451e-b7f9-c3021ad501cb",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨æ¨ç†æ¨¡å‹çš„å°è´´å£«\n",
    "\n",
    "è™½ç„¶æ¨ç†æ¨¡å‹åŠŸèƒ½å¼ºå¤§ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¹Ÿå¯èƒ½ç»™ç”¨æˆ·å¸¦æ¥å›°æ‰°ã€‚æœ¬èŠ‚æä¾›ä¸€äº›å®ç”¨å»ºè®®ï¼Œå¸®åŠ©ä½ æ›´å¥½åœ°é©¾é©­è¿™ç±»æ¨¡å‹ã€‚\n",
    "\n",
    "#### ä½¿ç”¨æ¸…æ™°ã€å‡†ç¡®çš„æç¤º\n",
    "\n",
    "é¢å¯¹æ¨ç†æ¨¡å‹æ—¶ï¼Œä¿æŒæç¤ºçš„æ¸…æ™°åº¦ä¸å‡†ç¡®æ€§å°¤ä¸ºé‡è¦ï¼Œè¿™èƒ½é¿å…æ¨¡å‹â€œç¥æ¸¸â€ï¼Œå¹¶ä¿ƒä½¿å®ƒè¾“å‡ºå¯¹è§£å†³ä»»åŠ¡çœŸæ­£æœ‰å¸®åŠ©çš„æ€è€ƒè¿‡ç¨‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0ec50-2d14-474b-8152-7a05eea1f621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A piece of code with no clear requirements on what the user expects\n",
    "query = \"\"\"\n",
    "def example(a):\n",
    "  b = []\n",
    "  for i in range(len(a)):\n",
    "    b.append(a[i]*2)\n",
    "  return sum(b)\n",
    "\"\"\"\n",
    "\n",
    "get_qwen_reasoning(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc6adc-ea08-45d3-b307-bf3bb4214a5b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The same piece of code with requirements clearly stated\n",
    "query = \"\"\"\n",
    "What does this python code do, and how can I simplify it? Be concise.\n",
    "def example(a):\n",
    "  b = []\n",
    "  for i in range(len(a)):\n",
    "    b.append(a[i]*2)\n",
    "  return sum(b)\n",
    "\"\"\"\n",
    "\n",
    "get_qwen_reasoning(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b66a9a-4394-45f1-a666-4dc019612f24",
   "metadata": {},
   "source": [
    "#### è°¨æ…ä½¿ç”¨é“¾å¼æ€ç»´æç¤º\n",
    "\n",
    "ç”±äºæ¨ç†æ¨¡å‹åœ¨â€œæ€è€ƒâ€é˜¶æ®µæœ¬èº«å°±å­˜åœ¨é“¾å¼æ€ç»´æˆåˆ†ï¼Œå†æ¬¡è¦æ±‚å®ƒæ‰§è¡Œé“¾å¼æ¨ç†ï¼Œå¯èƒ½å¯¼è‡´å¾ªç¯æˆ–å†—ä½™ã€‚éšç€ LLM ä¸æ–­å‡çº§ï¼Œè¿™ç±»é—®é¢˜æœ‰æœ›é€æ­¥è¢«ä¼˜åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468dc575-db7c-450d-ae7e-d0b7040f2b05",
   "metadata": {},
   "source": [
    "# å…ƒæç¤ºï¼šè®© LLM è‡ªåŠ¨å†™å‡ºæ›´å¥½çš„æç¤º\n",
    "\n",
    "---\n",
    "\n",
    "å…ƒæç¤ºï¼ˆMeta Promptingï¼‰æ˜¯ä¸€ç§æç¤ºæŠ€å·§ï¼šä½ å‘ LLM æé—®ï¼Œä½†é—®é¢˜çš„æ ¸å¿ƒæ˜¯â€œå¦‚ä½•å›ç­”æç¤ºâ€ã€‚æ¢è¨€ä¹‹ï¼Œå°±æ˜¯â€œå…³äºæç¤ºçš„æç¤ºâ€â€”â€”â€œmetaâ€ å³â€œè¶…è¶Šâ€æˆ–â€œå…³äºè‡ªèº«â€çš„æ„æ€ã€‚\n",
    "\n",
    "ä¸å…¶ç›´æ¥æé—®ï¼Œä¸å¦‚å…ˆæŒ‡å¯¼æ¨¡å‹å¦‚ä½•æ€è€ƒã€å¦‚ä½•ç»„ç»‡ç»“æ„ï¼Œå†ç»™å‡ºå®é™…ä»»åŠ¡ã€‚è¿™æ ·å¯ä»¥æ˜¾è‘—æå‡å›ç­”çš„è´¨é‡ã€å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§ã€‚\n",
    "\n",
    "## ä¸ºä»€ä¹ˆå®ƒæœ‰ç”¨\n",
    "\n",
    "ç¼–å†™é«˜è´¨é‡æç¤ºæ˜¯ä¸€é¡¹éœ€è¦è¿­ä»£çš„æŠ€èƒ½â€”â€”å³ä½¿æ˜¯ä¸“å®¶ä¹Ÿéš¾ä»¥ä¸€æ¬¡åˆ°ä½ã€‚é€šå¸¸æµç¨‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Write Initial Prompt] --> B[Run LLM]\n",
    "    B --> C[Analyze Output]\n",
    "    C --> D{Meets Quality?}\n",
    "    D -- No --> E[Generate Feedback via LLM Critic]\n",
    "    E --> F[Suggest Improvements]\n",
    "    F --> G[Rewrite Prompt]\n",
    "    G --> B\n",
    "    D -- Yes --> H[Deploy Optimized Prompt]\n",
    "```\n",
    "\n",
    "å…ƒæç¤ºçš„ä¼˜åŠ¿åœ¨äºï¼š\n",
    "\n",
    "* **åé¦ˆç³»ç»ŸåŒ–ï¼š** åˆ©ç”¨ LLM ä½œä¸ºç¨³å®šçš„â€œè£åˆ¤â€ï¼ˆLLM-as-a-judgeï¼‰ã€‚\n",
    "* **è¿­ä»£æ›´è¿…é€Ÿï¼š** ä»æƒ³æ³•åˆ°ä¼˜åŒ–åªéœ€æ•°ç§’ã€‚\n",
    "* **æç¤ºè¿›å…¥ CI/CDï¼š** åƒç®¡ç†ä»£ç ä¸€æ ·ç®¡ç†æç¤ºâ€”â€”æµ‹è¯•ã€è¯„åˆ†ã€ç‰ˆæœ¬åŒ–ã€éƒ¨ç½²ã€‚\n",
    "\n",
    "è¿™è®©æç¤ºå·¥ç¨‹æ‘†è„±â€œçŒœæµ‹å¼è°ƒå‚â€ï¼Œè½¬è€Œæˆä¸º**å¯é‡å¤ã€å¯æ‰©å±•çš„å·¥ä½œæµç¨‹**ï¼Œå¯¹ç”Ÿäº§çº§ AI ç³»ç»Ÿè‡³å…³é‡è¦ã€‚\n",
    "\n",
    "ä¸ºäº†æ·±å…¥ç†è§£å…ƒæç¤ºçš„å¨åŠ›ï¼Œæˆ‘ä»¬é©¬ä¸ŠåŠ¨æ‰‹å®è·µã€‚\n",
    "\n",
    "> **æç¤ºï¼š** ä¸ºç¡®ä¿ç¤ºä¾‹ä¸€è‡´ï¼Œä¸‹é¢æˆ‘ä»¬å°†æ¨¡æ‹Ÿå›ºå®šçš„æ£€ç´¢ä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "## ç¬¬ä¸€æ­¥ï¼šä½ çš„â€œåˆå§‹â€æç¤º\n",
    "\n",
    "æ‰€æœ‰ä¼˜åŒ–éƒ½æœ‰èµ·ç‚¹ã€‚æ‰€è°“â€œåˆå§‹æç¤ºâ€é€šå¸¸ç»“æ„æ¾æ•£ã€ç¼ºä¹è¯­æ°”å’Œæ ¼å¼è¦æ±‚ã€‚\n",
    "\n",
    "å®ƒå¯ä»¥â€œå¾ˆç³Ÿç³•â€â€”â€”è¿™æ­£æ˜¯æˆ‘ä»¬éœ€è¦æ”¹è¿›å®ƒçš„åŸå› ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b8f17-8729-42c1-9b9c-d2a6b7ded67e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context  = \"\"\"\n",
    "    TaskFriend is a productivity assistant that helps users manage tasks, calendars, and notes.\n",
    "    It supports task prioritization, deadline tracking, and integration with external calendars like Google Calendar and Outlook.\n",
    "    Users can add tasks via voice, text, or email. TaskFriend automatically sorts tasks by due date and priority.\n",
    "    It also provides daily summaries and weekly planning tools to help users stay on top of their workload.\n",
    "\"\"\"\n",
    "\n",
    "first_query = f\"\"\"\n",
    "    Based on the following information, describe in what TaskFriend is and what it does in a paragraph.\n",
    "\n",
    "    [Reference Info]\n",
    "    {context}\n",
    "\"\"\"\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"'First' (naive) answer\")\n",
    "print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "first_response = get_qwen_stream_response(first_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdaabf0-8879-4a8f-a868-1de997b25663",
   "metadata": {},
   "source": [
    "**ä½ å¯èƒ½å¾—åˆ°çš„ç»“æœï¼š**  \n",
    "ç»“æ„å¹³æ·¡ã€ç¼ºä¹å±‚æ¬¡â€”â€”å¾€å¾€æ˜¯ä¸€æ•´æ®µæ²¡æœ‰è§†è§‰å…ƒç´ çš„æ–‡å­—ï¼Œä¸ä¸Šä¸‹æ–‡å‡ ä¹æ²¡æœ‰å·®å¼‚ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "> `TaskFriend is a comprehensive productivity assistant designed to help users efficiently manage their tasks, calendars, and notes. It enables users to add tasks through voice, text, or email, and automatically organizes them by due date and priority, ensuring important deadlines are never missed. With integrations for popular external calendars like Google Calendar and Outlook, TaskFriend streamlines scheduling and time management. Additionally, it offers daily summaries and weekly planning tools to keep users informed and prepared, making it easier to stay on top of their workload and maintain productivity.`\n",
    "\n",
    "**ä¸ºä»€ä¹ˆé‡è¦ï¼š**  \n",
    "è¿™æ˜¯åŸºçº¿ã€‚è®¸å¤šäººä¼šæ­¢æ­¥äºæ­¤ã€‚ä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™æ°æ°æ˜¯â€œçœŸæ­£å·¥ç¨‹å·¥ä½œâ€çš„èµ·ç‚¹ã€‚\n",
    "\n",
    "> **ğŸ’¡ æ ¸å¿ƒæ´è§ï¼š**  \n",
    "> ç²—ç³™çš„æç¤ºåªä¼šäº§ç”Ÿç²—ç³™çš„è¾“å‡ºã€‚è‹¥æƒ³ä¼˜åŒ–ç»“æœï¼Œå¿…é¡»å…ˆç•Œå®šâ€œæ›´å¥½â€æ„å‘³ç€ä»€ä¹ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1940359-dc13-4de2-b21a-d16505877548",
   "metadata": {},
   "source": [
    "åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»ä¸€ä¸ªç®€å•çš„æç¤ºå¼€å§‹ï¼Œå¹¶è¿ç”¨è¿™å››ä¸ªæ­¥éª¤é€æ­¥æ”¹è¿›å®ƒã€‚è¿™æ˜¯ä¸€ä¸ªå¸¸è§ä½†ç»“æ„æ¾æ•£çš„ TaskFriend åŠŸèƒ½æç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535b674-6af8-4059-936f-9663de45b580",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_prompt = f\"\"\"\n",
    "    [Role] You are a prompt engineering expert. \n",
    "    [Task] Your job is to help me write a prompt based on the instructions below.\n",
    "\n",
    "    [Original prompt]\n",
    "    ---\n",
    "    {first_query}\n",
    "    ---\n",
    "\n",
    "    [Generated response]\n",
    "    ---\n",
    "    {first_response}\n",
    "    ---\n",
    "\n",
    "    [Instructions]\n",
    "    The response is too basic. I want the assistant to sound professional yet friendly, and to clearly explain TaskFriendâ€™s value to a new user.\n",
    "\n",
    "    Specific improvements the prompt needs to achieve:\n",
    "    1. **Tone**: Friendly, encouraging, and helpful â€” like a real productivity buddy.\n",
    "    2. **Structure**: Use bullet points or categories (e.g., \"What It Is\", \"Key Features\", \"Why It Helps\").\n",
    "    3. **Visual Appeal**: Use emojis to highlight features and draw attention.\n",
    "\n",
    "    Output only the prompt, do not output any explainations or context.\n",
    "\"\"\"\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Optimized prompt\")\n",
    "print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "optimized_prompt = get_qwen_stream_response(meta_prompt)\n",
    "print(optimized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5b515-c2f6-4be4-95e9-37672aa3fe4b",
   "metadata": {},
   "source": [
    "è®©æˆ‘ä»¬ä»…è¿è¡Œ `run_assistant()` æ¥æŸ¥çœ‹ç»“æœï¼š\n",
    "\n",
    "{% raw %}\n",
    "```\n",
    "\n",
    "(((<some generic output>)))\n",
    "```\n",
    "\n",
    "{% endraw %}\n",
    "\n",
    "ç»“æœæ¯”è¾ƒä¸€èˆ¬ï¼Œå¯ä»¥çœ‹åˆ°ï¼š\n",
    "\n",
    "- è¾“å‡ºä¸å®Œæ•´â€”â€”åªå¥½åƒæ˜¯æŸä¸ªé•¿æ®µè½çš„å¼€å¤´ã€‚\n",
    "- è¡¨è¿°éå¸¸ç¬¼ç»Ÿã€‚\n",
    "- å…¨ç¨‹æ²¡æœ‰ç»“æ„æˆ–é¡¹ç›®ç¬¦å·ã€‚\n",
    "\n",
    "æˆ‘ä»¬æŠŠè¿™å½“æˆèµ·ç‚¹ã€‚å¾ˆå¤šæ—¶å€™ï¼Œä½ è®¾è®¡æç¤ºæ—¶ä¼šè§‰å¾—â€œè¿˜è¡Œâ€â€”â€”ä½†ç³»ç»Ÿæç¤ºæœªåšä»»ä½•æ”¹è¿›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b8747-d9f0-4335-a855-5f750643d577",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 2ï¼šè®¾è®¡æç¤ºç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f8527-1ee9-4fb8-bfba-0e85236e4d39",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_prompt = f\"\"\"\n",
    "    Based on the following information, describe TaskFriend in a way thatâ€™s professional yet friendly â€” like a helpful productivity buddy. \n",
    "    Organize your response into clear sections such as **\"What It Is\"**, **\"Key Features\"**, and **\"Why It Helps\"**.\n",
    "    Use bullet points or emojis (like ğŸ“…, â°, ğŸ¯) to make it visually engaging and highlight important features. \n",
    "    Make sure new users clearly understand what TaskFriend does and why it's valuable for staying organized and productive.\n",
    "\n",
    "    [Reference Info]\n",
    "    {context}\n",
    "\"\"\"\n",
    "print(\"-\" * 50)\n",
    "print(\"Final answer (with optimized prompt)\")\n",
    "print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "final_answer = get_qwen_stream_response(final_prompt)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967eb768-0c16-4456-8549-f4e062f1a746",
   "metadata": {},
   "source": [
    "å°±åƒæˆ‘ä»¬åœ¨ä¸Šä¸€ç« æ„å»ºå¤šè½®å¯¹è¯æ—¶æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæç¤ºæ¨¡æ¿æ¥æ”¹è¿›ç»“æ„ã€å˜é‡ä»¥åŠæˆ‘ä»¬ä¼ é€’ç»™æ¨¡å‹çš„ä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "æˆ‘ä»¬ä»ä¸€ä¸ªæ›´å¼ºçš„æç¤ºæ¨¡æ¿å¼€å§‹ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59292a05-97dc-4617-8b82-bab5c8a2a1ee",
   "metadata": {},
   "source": [
    "è¿™é‡Œæˆ‘ä»¬åšäº†å‡ ä»¶äº‹ï¼š\n",
    "\n",
    "- **ä»¥æ¸…æ™°çš„ç»“æ„å¼€å§‹ï¼Œå¼ºè°ƒæˆ‘ä»¬æƒ³è¦çš„å†…å®¹**ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æ˜ç¡®ä¼ è¾¾ï¼šæˆ‘ä»¬å¸Œæœ›å¾—åˆ°ä¸€ä¸ªè¥é”€ç®€ä»‹ï¼Œå®ƒåˆ†ä¸ºâ€œæ¦‚è¿°â€å’Œâ€œå…³é”®åŠŸèƒ½â€ä¸¤ä¸ªéƒ¨åˆ†ã€‚\n",
    "- æˆ‘ä»¬**æä¾›å¤šä¸ªè¦ç´ çš„å®šä¹‰**ï¼šæˆ‘ä»¬åŒ…æ‹¬äº§å“æˆ–åŠŸèƒ½çš„åç§°ã€ç”¨æˆ·è§’è‰²ã€åˆ†ç±»ï¼Œä»¥åŠæˆ‘ä»¬éœ€è¦æ¶µç›–çš„å…³é”®åŠŸèƒ½ã€‚è¿™äº›è¦ç´ ä¸­çš„å¾ˆå¤šæ¥è‡ªæˆ‘ä»¬è®°å½•çš„è¾“å…¥ã€‚\n",
    "- æˆ‘ä»¬**ç•™æœ‰ä¸€ä¸ªâ€œç”¨æˆ·è¯·æ±‚â€æ§½ä½**ï¼šè¿™æ ·æˆ‘ä»¬å¯ä»¥å°†ç”¨æˆ·çš„è¾“å…¥ä¼ é€’è¿›å»ã€‚\n",
    "- æˆ‘ä»¬**å£°æ˜æœŸæœ›çš„è¾“å‡ºæ ¼å¼**ï¼šç¬¬ä¸€é¡¹è¦æ±‚ä¸ºä¸¤åˆ°ä¸‰æ®µï¼Œå¹¶é™„å¸¦é¡¹ç›®ç¬¦å·åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92eb5ba-f156-4e61-8bd7-3329659c520c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from functions.clean_json import clean_json_response\n",
    "\n",
    "reference_answer = \"\"\"\n",
    "ğŸ‘‹ Meet TaskFriend â€” Your Personal Productivity Partner!\n",
    "\n",
    "ğŸ“Œ **What It Is**  \n",
    "TaskFriend is a smart productivity assistant that helps you manage tasks, calendars, and personal notes â€” all in one place.\n",
    "\n",
    "âš™ï¸ **Key Features**  \n",
    "- ğŸ“… Syncs with Google Calendar and Outlook  \n",
    "- ğŸ“ Takes notes and turns them into actionable tasks  \n",
    "- ğŸ§  Prioritizes your to-dos based on urgency and importance  \n",
    "- ğŸ—£ï¸ Supports voice, text, and email input  \n",
    "- ğŸ“‹ Daily summaries and weekly planning tools  \n",
    "\n",
    "âœ¨ **Why It Helps**  \n",
    "TaskFriend keeps your workflow smooth and your mind clear â€” so you can focus on what matters most.\n",
    "\"\"\"\n",
    "\n",
    "def get_qwen_response(query):\n",
    "    full_response = \"\"\n",
    "    for token in get_qwen_stream_response(query):\n",
    "        full_response += token\n",
    "    return full_response\n",
    "\n",
    "def analyze_gap_quick(generated_response, reference):\n",
    "    gap_prompt = f\"\"\"\n",
    "    [Role] You are a prompt evaluation assistant.\n",
    "    [Task] Compare the generated response to the reference answer. Be sensitive to improvements.\n",
    "\n",
    "    [Instructions]\n",
    "    - Compare ONLY the generated response and reference below.\n",
    "    - Score each dimension 1â€“5 (5 = perfect match to reference in style and substance).\n",
    "    - Focus on visual layout, emoji use, section headers, conciseness, and scannability.\n",
    "    - If the current response better matches the reference than before, reflect that in higher scores.\n",
    "\n",
    "    [Scoring Rubric]\n",
    "    - tone: Friendly, upbeat, conversational (like a helpful friend). Emojis enhance warmth.\n",
    "    - structure: Sections titled exactly or similarly: \"What It Is\", \"Key Features\", \"Why It Helps\"\n",
    "    - content: All key features and benefits present without extra fluff\n",
    "    - visual: Uses emojis, bolding, bullet points, spacing like the reference â€” clean and easy to scan\n",
    "\n",
    "    [Critical] Re-evaluate completely. Do not repeat previous scores. Even small improvements matter.\n",
    "\n",
    "    [Reference Answer]\n",
    "    {reference}\n",
    "\n",
    "    [Generated Response]\n",
    "    {generated_response}\n",
    "\n",
    "    [Output Format]\n",
    "    Return a JSON object like:\n",
    "    {{\n",
    "      \"tone\": int,\n",
    "      \"structure\": int,\n",
    "      \"content\": int,\n",
    "      \"visual\": int\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return get_qwen_response(gap_prompt)\n",
    "\n",
    "def suggest_prompt_improvements(current_prompt, gap_scores, reference):\n",
    "    improvement_prompt = f\"\"\"\n",
    "    [Role] You are a prompt engineering expert.\n",
    "    [Task] Based on the gap scores, suggest specific changes to elements that score <4 to improve alignment with the reference answer.\n",
    "\n",
    "    [Current Prompt]\n",
    "    {current_prompt}\n",
    "\n",
    "    [Gap Scores]\n",
    "    {json.dumps(gap_scores)}\n",
    "    \n",
    "    [Reference Answer]\n",
    "    {reference}\n",
    "\n",
    "    [Instructions]\n",
    "    Return a list of actionable suggestions, be short and concise, 5 actions max. (e.g., \"Add 'use emojis' to instructions\", \"Clarify feature list\").\n",
    "    \"\"\"\n",
    "    return get_qwen_response(improvement_prompt)\n",
    "\n",
    "def improve_prompt_with_suggestions(current_prompt, suggestions):\n",
    "    improvement_application_prompt = f\"\"\"\n",
    "    [Role] You are a prompt engineer.\n",
    "    [Task] Improve the following prompt based on the given suggestions.\n",
    "\n",
    "    [Current Prompt]\n",
    "    {current_prompt}\n",
    "\n",
    "    [Suggestions]\n",
    "    {suggestions}\n",
    "\n",
    "    [Instructions]\n",
    "    Return the improved prompt only â€” no explanation.\n",
    "    \"\"\"\n",
    "    return get_qwen_response(improvement_application_prompt)\n",
    "\n",
    "current_prompt = first_query  # Make sure this is defined earlier\n",
    "prompt_history = []  # Optional: track prompt evolution\n",
    "\n",
    "for i in range(3):  # Max 3 iterations\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Get and print the full streamed response\n",
    "    generated_response = get_qwen_stream_response(current_prompt)\n",
    "    print(current_prompt)\n",
    "    print(f\"\\n\\nGenerated Response (sample):\\n{generated_response[:250]}...\")\n",
    "\n",
    "    # Analyze gap with lightweight scoring\n",
    "    gap_report = analyze_gap_quick(generated_response, reference_answer)\n",
    "\n",
    "    # Clean the response before parsing\n",
    "    cleaned_report = clean_json_response(gap_report)\n",
    "\n",
    "    try:\n",
    "        gap_scores = json.loads(cleaned_report)\n",
    "        print(\"\\nâœ… Parsed gap scores:\", gap_scores)\n",
    "\n",
    "        # Check for convergence\n",
    "        if all(score >= 4 for score in gap_scores.values()):\n",
    "            print(\"\\nâœ… Evaluation passed. Output quality is sufficient.\")\n",
    "            break\n",
    "\n",
    "        # Get improvement suggestions\n",
    "        print(\"\\nğŸ” Generating prompt improvement suggestions...\")\n",
    "        print(\"Suggested Improvements:\\n\")\n",
    "        improvement_suggestions = suggest_prompt_improvements(current_prompt, gap_scores, reference_answer)\n",
    "        print(improvement_suggestions)\n",
    "        print(\"\\n\")\n",
    "              \n",
    "        # Apply suggestions to current prompt\n",
    "        print(\"\\nğŸ› ï¸ Applying suggestions to improve the prompt...\\n\")\n",
    "        print(\"Improved prompt:\")\n",
    "        current_prompt = improve_prompt_with_suggestions(current_prompt, improvement_suggestions)\n",
    "        print(current_prompt)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Optional: Store in history\n",
    "        prompt_history.append({\n",
    "            \"iteration\": i+1,\n",
    "            \"prompt\": current_prompt,\n",
    "            \"gap_scores\": gap_scores\n",
    "        })\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"[Error] Could not parse gap scores. Skipping convergence check.\")\n",
    "        print(\"Raw gap report:\", gap_report)\n",
    "        continue\n",
    "else:\n",
    "    print(\"\\nğŸ” Max iterations reached. Optimization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f8a57-2bdd-4d46-a4ac-00264b4139c6",
   "metadata": {},
   "source": [
    "è¿™å·²ç»æ¯”ä¹‹å‰å¥½å¾ˆå¤šäº†ï¼åº†ç¥ä¸€ä¸‹â€”â€”åœ¨æç¤ºå·¥ç¨‹ä¸­ä¿æŒåŠ¨åŠ›å¾ˆé‡è¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f309f6-815d-4463-b5ea-8a8b072e3332",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functions.grader_plot import plot_grader\n",
    "import re\n",
    "\n",
    "\n",
    "# Test Responses\n",
    "bad_response = \"\"\"\n",
    "**TaskFriend** is a productivity assistant designed to help users efficiently manage their time.\n",
    "It offers a range of features aimed at improving organization and time management:\n",
    "\n",
    "- **Task Management**\n",
    "- **Deadline Tracking**\n",
    "- **Calendar Integration**\n",
    "- **Planning & Summaries**\n",
    "\n",
    "Overall, TaskFriend serves as a comprehensive organizational tool that helps users streamline \n",
    "their workflow, reduce missed deadlines, and maintain better control over their daily and \n",
    "long-term tasks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ok_response = \"\"\"\n",
    "ğŸ‘‹ I'm **TaskFriend**, your personal productivity partner â€” nice to meet you! ğŸ‰\n",
    "\n",
    "Iâ€™m here to help you take control of your tasks, calendars, and notes â€” all in one smart, \n",
    "easy-to-use place. Letâ€™s make sure youâ€™re always working on what matters most, without the stress.  \n",
    "\n",
    "Hereâ€™s how I can help boost your productivity:\n",
    "\n",
    "### ğŸ“… Smart Scheduling  \n",
    "Sync seamlessly with **Google Calendar** and **Outlook**, so your schedule is always up-to-date and at your fingertips.\n",
    "\n",
    "### ğŸ“ Flexible Task Entry  \n",
    "Add tasks using:\n",
    "- âœï¸ Text\n",
    "- ğŸ—£ï¸ Voice input\n",
    "- ğŸ“§ Email integration  \n",
    "Whatever fits your style best!\n",
    "\n",
    "### ğŸ” Automatic Prioritization  \n",
    "Let me sort your to-dos by **due date** and **importance**, so you know exactly where to focus each day.\n",
    "\n",
    "### ğŸ§  Daily & Weekly Planning Tools  \n",
    "Start your day strong with a clear plan and get ahead on the week â€” Iâ€™ll help you stay strategic and organized.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Re-use previous reference answer, exact match\n",
    "perfect_response = reference_answer\n",
    "\n",
    "# Grade Response Quality\n",
    "def grade_response_detailed(response_to_grade, reference=reference_answer):\n",
    "    grader_prompt = f\"\"\"\n",
    "    [Role] You are a prompt evaluation assistant.\n",
    "    [Task] Compare the generated response to the reference answer. Be sensitive to improvements.\n",
    "    \n",
    "    [Instructions] Maximum score for each metric is 5, while the minimum is 1\n",
    "\n",
    "    [Reference answer]\n",
    "    {reference}\n",
    "\n",
    "    [Generated response]\n",
    "    {response_to_grade}\n",
    "\n",
    "    [Output Format]\n",
    "    Return only a JSON object like:\n",
    "    {{\n",
    "      \"tone\": int,\n",
    "      \"structure\": int,\n",
    "      \"content\": int,\n",
    "      \"visual\": int\n",
    "    }}\n",
    "    \n",
    "    Do not return anything else\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw_output = get_qwen_response(grader_prompt)\n",
    "\n",
    "        # Extract JSON silently\n",
    "        json_match = re.search(r'\\{[^}]+\\}', raw_output, re.DOTALL)\n",
    "        if json_match:\n",
    "            cleaned = json_match.group(0)\n",
    "            result = json.loads(cleaned)\n",
    "            # Clamp scores to 1â€“5 (fallback measure)\n",
    "            return {k: max(1, min(5, v)) for k, v in result.items()}\n",
    "        else:\n",
    "            return {\"tone\": 1, \"structure\": 1, \"content\": 1, \"visual\": 1}\n",
    "    except Exception:\n",
    "        return {\"tone\": 1, \"structure\": 1, \"content\": 1, \"visual\": 1}\n",
    "\n",
    "# ğŸ“Š Evaluate Responses\n",
    "scores = {\n",
    "    \"Initial Prompt\": grade_response_detailed(bad_response),\n",
    "    \"Optimized Prompt\": grade_response_detailed(ok_response),\n",
    "    \"Final Prompt\": grade_response_detailed(perfect_response),\n",
    "}\n",
    "\n",
    "# ğŸ“ˆ Plot Results\n",
    "plot_grader(\n",
    "    scores,\n",
    "    title=\"Prompt Evolution: Response Quality Over Iterations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9aef0-53ad-45cc-b5f9-a3603ca736bf",
   "metadata": {},
   "source": [
    "<b>1. åœ¨ RAG ç³»ç»Ÿä¸­ï¼Œæç¤ºæ¨¡æ¿çš„ä¸»è¦ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿ</b>  \n",
    "\n",
    "<ul>\n",
    "    <li>A) å­˜å‚¨æ£€ç´¢åˆ°çš„æ–‡æ¡£    </li>\n",
    "    <li>B) å®šä¹‰ AI çš„äººæ ¼ç‰¹è´¨    </li>\n",
    "    <li>C) æŒ‡ç¤º LLM å¦‚ä½•åˆ©ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å›ç­”æŸ¥è¯¢    </li>\n",
    "    <li>D) å–ä»£ç³»ç»Ÿæç¤º</li>\n",
    "</ul>\n",
    "\n",
    "**æŸ¥çœ‹ç­”æ¡ˆ â†’**\n",
    "</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
    "\n",
    "âœ… **æ­£ç¡®ç­”æ¡ˆï¼š** C) æŒ‡ç¤º LLM å¦‚ä½•åˆ©ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å›ç­”æŸ¥è¯¢  \n",
    "ğŸ“ **è¯´æ˜**ï¼š\n",
    "* æç¤ºæ¨¡æ¿é€šè¿‡ç»„åˆæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·æŸ¥è¯¢æ¥ç»„ç»‡ä¼ å…¥ LLM çš„è¾“å…¥ï¼Œç¡®ä¿æ¨¡å‹åŸºäºæä¾›çš„æ•°æ®ä½œç­”ï¼Œè€Œä¸æ˜¯ä¾èµ–å…¶æ—¢æœ‰çŸ¥è¯†ã€‚\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "## è¦ç‚¹å›é¡¾\n",
    "\n",
    "* **ä»€ä¹ˆæ˜¯ä¼˜è´¨æç¤º**\n",
    "    * **ç³»ç»Ÿæç¤ºå°±æ˜¯ AI çš„å²—ä½è¯´æ˜ä¹¦**â€”â€”å®ƒç•Œå®šè§’è‰²ã€ç›®æ ‡ã€å—ä¼—ã€è¾“å‡ºæ ¼å¼ä»¥åŠå®‰å…¨æŠ¤æ ã€‚\n",
    "    * **å¼ºæç¤ºå¯ä»¥é˜²æ­¢åèˆª**â€”â€”æ²¡æœ‰æ˜ç¡®æŒ‡ä»¤æ—¶ï¼ŒLLM å®¹æ˜“ç»™å‡ºæ³›æ³›æˆ–è·‘é¢˜çš„å›åº”ã€‚\n",
    "    * **ä½¿ç”¨ R-G-C-A-F-G æ¡†æ¶**ï¼š\n",
    "      - **Roleï¼ˆè§’è‰²ï¼‰**ï¼šAI æ˜¯è°ï¼ˆä¾‹å¦‚ï¼šâ€œTaskFriendï¼Œä¸€åæ•ˆç‡åŠ©ç†â€ï¼‰\n",
    "      - **Goalï¼ˆç›®æ ‡ï¼‰**ï¼šå®ƒè¦è¾¾æˆä»€ä¹ˆï¼ˆä¾‹å¦‚ï¼šâ€œå¸®åŠ©ç”¨æˆ·ä¸ºä»»åŠ¡æ’ä¼˜å…ˆçº§â€ï¼‰\n",
    "      - **Contextï¼ˆä¸Šä¸‹æ–‡ï¼‰**ï¼šå®ƒå¯ä»¥ä½¿ç”¨å“ªäº›æ•°æ®ï¼ˆä¾‹å¦‚ï¼šâ€œé€šè¿‡ä»»åŠ¡åˆ—è¡¨çš„ RAG æ£€ç´¢â€ï¼‰\n",
    "      - **Audienceï¼ˆå—ä¼—ï¼‰**ï¼šå®ƒåœ¨ä¸è°å¯¹è¯ï¼ˆä¾‹å¦‚ï¼šâ€œå¿™ç¢Œçš„èŒåœºäººå£«â€ï¼‰\n",
    "      - **Formatï¼ˆæ ¼å¼ï¼‰**ï¼šå®ƒåº”è¯¥å¦‚ä½•å›åº”ï¼ˆä¾‹å¦‚ï¼šâ€œä½¿ç”¨é¡¹ç›®ç¬¦å·ï¼Œä¸è¦ Markdownâ€ï¼‰\n",
    "      - **Guardrailsï¼ˆæŠ¤æ ï¼‰**ï¼šå®ƒå¿…é¡»é¿å…ä»€ä¹ˆï¼ˆä¾‹å¦‚ï¼šâ€œç¦æ­¢è§’è‰²æ‰®æ¼”ï¼Œä¸å‘è¡¨ä¸ªäººæ„è§â€ï¼‰\n",
    "    * **æç¤ºæ¨¡æ¿çš„ä½œç”¨ä¸åŒ**â€”â€”å®ƒä»¬çº¦æŸçš„æ˜¯ LLM å¦‚ä½•åˆ©ç”¨**æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡**ï¼Œè€Œéå®ƒçš„èº«ä»½ã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "* **ç”¨æç¤ºæ¨¡æ¿è¿æ¥ä¸Šä¸‹æ–‡ä¸æŸ¥è¯¢**\n",
    "    * **æç¤ºæ¨¡æ¿æ˜¯ RAG çš„å¼•æ“**â€”â€”å®ƒæŒ‡ç¤º LLM å¦‚ä½•åˆ©ç”¨æ£€ç´¢æ–‡æ¡£ä½œç­”ã€‚\n",
    "    * **å®ƒä»¬å¼ºåŒ–åŸºäºä¾æ®çš„è¾“å‡º**â€”â€”ç±»ä¼¼â€œè¯·åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯ä½œç­”ï¼Œä¸è¦å¼•ç”¨æ—¢æœ‰çŸ¥è¯†â€çš„è¯­å¥èƒ½å‡å°‘å¹»è§‰ã€‚\n",
    "    * **å®ƒä»¬æ ‡å‡†åŒ–è¾“å…¥ç»“æ„**â€”â€”è®©æç¤ºæ›´å¯é¢„æµ‹ã€æ›´æ˜“æµ‹è¯•ã€‚\n",
    "    * **ä¼˜ç§€çš„æ¨¡æ¿åŒ…å«ä»¥ä¸‹è¦ç´ **ï¼š\n",
    "      - åœ¨ä¸Šä¸‹æ–‡ä¸æŸ¥è¯¢ä¹‹é—´åšæ¸…æ™°åˆ†éš”ï¼ˆä¾‹å¦‚ `---`ï¼‰\n",
    "      - æŒ‡ç¤ºå¿½ç•¥æ—¢æœ‰çŸ¥è¯†\n",
    "      - è¾“å‡ºæ ¼å¼è§„åˆ™ï¼ˆä¾‹å¦‚ JSONã€é¡¹ç›®ç¬¦å·ï¼‰\n",
    "    * **å®ƒä»¬å¹¶éä¸‡æ— ä¸€å¤±**â€”â€”ç”¨æˆ·æŸ¥è¯¢ä»å¯èƒ½è¦†ç›–æ¨¡æ¿æŒ‡ä»¤ï¼Œå› æ­¤è¦æ˜ç¡®è®¾è®¡è¾¹ç•Œã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "* **æ„å»ºæ›´æ™ºèƒ½çš„æç¤ºæ¨¡æ¿**\n",
    "    * **æ ¼å¼åŒ–æå‡æ¸…æ™°åº¦**â€”â€”ä½¿ç”¨ `# æ ‡é¢˜`ã€`---`ã€`[æ–¹æ‹¬å·]` å’Œ `**åŠ ç²—**` å¼•å¯¼æ¨¡å‹ã€‚\n",
    "    * **è¡¨æƒ…å’Œè§†è§‰æç¤ºæ”¹å–„ä½“éªŒ**â€”â€”è®©è¾“å‡ºæ›´æ˜“è¯»ã€æ›´æœ‰å¸å¼•åŠ›ã€‚\n",
    "    * **ç»“æ„åŒ–è¾“å‡ºè‡³å…³é‡è¦**â€”â€”æç¤ºæ¨¡æ¿å¯å¼ºåˆ¶ç”Ÿæˆ JSONã€è¡¨æ ¼æˆ–ä»£ç ï¼Œä¾¿äºç”Ÿäº§ç³»ç»Ÿè§£æã€‚\n",
    "    * **æç¤ºæ¨¡æ¿æ˜¯å¼€å‘è€…å·¥å…·**â€”â€”å®ƒä»¬æå‡ä¸€è‡´æ€§ï¼Œä½†æ— æ³•å®Œå…¨é˜»æ­¢ç”¨æˆ·è¾“å…¥çš„å½±å“ã€‚\n",
    "    * **å®ƒä»¬è®© RAG æ›´å¯é **â€”â€”æ²¡æœ‰æ¨¡æ¿æ—¶ï¼Œæ¨¡å‹å¯èƒ½å¿½è§†ä¸Šä¸‹æ–‡æˆ–é€€å›åˆ°è®­ç»ƒæ•°æ®ã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "* **é€šç”¨æç¤ºå·¥ç¨‹æŠ€å·§**\n",
    "\n",
    "    * **è§’è‰²/äººè®¾æç¤º**å¯æ”¹å˜è¯­æ°”ä¸æ·±åº¦â€”â€”ä¾‹å¦‚â€œç”¨ 5 å²å„¿ç«¥å¬å¾—æ‡‚çš„æ–¹å¼è§£é‡Šâ€ä¸â€œä»¥èµ„æ·±å·¥ç¨‹å¸ˆçš„è§’åº¦è¯´æ˜â€ã€‚\n",
    "    * **å°‘æ ·æœ¬ç¤ºèŒƒ**ç”¨ç¤ºä¾‹æ•™æˆæ ¼å¼å’Œé£æ ¼â€”â€”å“ªæ€•åªæœ‰ä¸€ä¸ªç¤ºä¾‹ï¼Œä¹Ÿèƒ½æ˜¾è‘—æå‡è¾“å‡ºä¸€è‡´æ€§ã€‚\n",
    "    * **é“¾å¼æ€è€ƒï¼ˆCoTï¼‰**èƒ½æå‡æ¨ç†èƒ½åŠ›â€”â€”è®©æ¨¡å‹â€œå±•ç¤ºæ¨ç†è¿‡ç¨‹â€å¾€å¾€èƒ½å¾—åˆ°æ›´å¥½çš„ç­”æ¡ˆã€‚\n",
    "    * **æ¨ç†æ¨¡å‹ï¼ˆå¦‚å¯ç”¨æ€è€ƒæ¨¡å¼çš„ Qwen-plusï¼‰**æ›´è¿›ä¸€æ­¥â€”â€”å®ƒä»¬ä¼šåœ¨å›ç­”å‰è‡ªåŠ¨ç”Ÿæˆå†…éƒ¨æ¨ç†ã€‚\n",
    "    * **åœ¨æ¨ç†æ¨¡å‹ä¸Šé¿å…å¼ºåˆ¶ CoT æç¤º**â€”â€”å¯èƒ½é€ æˆå¾ªç¯æ€è€ƒï¼›æ¨¡å‹å·²åœ¨å†…éƒ¨è¿›è¡Œæ¨ç†ã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "* **å…ƒæç¤ºä¸è‡ªåŠ¨åŒ–è¯„ä¼°**\n",
    "    * **å…ƒæç¤º**å³â€œå›´ç»•æç¤ºæœ¬èº«è¿›è¡Œæç¤ºâ€â€”â€”åˆ©ç”¨ LLM æ¥å®¡æŸ¥å¹¶æ”¹è¿›ä½ çš„æç¤ºã€‚\n",
    "    * **å®ƒèƒ½åŠ é€Ÿè¿­ä»£**â€”â€”æ— éœ€åå¤äººå·¥è¯•é”™ï¼Œè€Œæ˜¯è‡ªåŠ¨åŒ–ä¼˜åŒ–å¾ªç¯ã€‚\n",
    "    * **å·®è·åˆ†æ**é€šè¿‡ LLM è¯„å®¡å°†è¾“å‡ºä¸å‚è€ƒç­”æ¡ˆå¯¹æ¯”ã€‚\n",
    "    * **è‡ªåŠ¨è¯„åˆ†**ä½¿ç”¨è¯„åˆ†é‡è¡¨ï¼ˆè¯­æ°”ã€ç»“æ„ã€å†…å®¹ã€è§†è§‰ï¼‰é‡åŒ–è´¨é‡ã€‚\n",
    "    * **è®©æç¤ºä¹Ÿèƒ½ CI/CD**â€”â€”åƒä»£ç ä¸€æ ·æµ‹è¯•ã€æ‰“åˆ†å’Œå‘å¸ƒæç¤ºæ›´æ–°ï¼Œç¡®ä¿å¤§è§„æ¨¡çš„ä¸€è‡´è´¨é‡ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineer (Professional)",
   "language": "python",
   "name": "llm_pro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
