{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd3a3b0-80b4-4774-8e5e-3d09da21211e",
   "metadata": {},
   "source": [
    "# Bridging Knowledge with Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "---\n",
    "\n",
    "Now that our LLM can remember key decisions and maintain coherent conversations, it's time to give it **real knowledge** ‚Äî about *your* tasks, *your* calendar, *your* project docs.\n",
    "\n",
    "Right now, your **TaskFriend** app is limited to what it hears during the chat. But what if it could:\n",
    "\n",
    "* Pull in your upcoming calendar events?\n",
    "* Read your task list from a database?\n",
    "* Reference your company‚Äôs onboarding guide?\n",
    "\n",
    "That‚Äôs where **Retrieval-Augmented Generation (RAG)** comes in.\n",
    "\n",
    "With RAG, your LLM doesn‚Äôt just rely on pre-trained knowledge ‚Äî it retrieves relevant information from external sources and uses it to generate accurate, personalized responses.\n",
    "\n",
    "\n",
    "## The story so far...\n",
    "\n",
    "**Scenario:**\n",
    "Users love chatting with **TaskFriend** as they can work with it to figure out how to plan their day. The features you've built so far lke streaming responses, multi-turn conversations, and a professional system prompt have been a great hit! However, there's still a gap:\n",
    "\n",
    "**TaskFriend** is unable to provide satisfactory answers to questions like:\n",
    "\n",
    "> ‚ÄúWhat tasks do I have due this week?‚Äù\n",
    "\n",
    "Or:\n",
    "\n",
    "> ‚ÄúCan I reschedule my presentation prep if I go to the gym tomorrow morning?‚Äù\n",
    "\n",
    "This is not because **TaskFriend** is not smart enough, but because it has **no access to the user‚Äôs actual data**. It remembers what was *said* in the conversation, but not what‚Äôs *true* in the user‚Äôs world. \n",
    "\n",
    "## Goals\n",
    "\n",
    "* Understand how RAG extends LLM knowledge beyond pre-training\n",
    "* Build a document retrieval system using embeddings and vector search\n",
    "* Inject retrieved context into prompts to generate informed responses\n",
    "* Handle private, dynamic, or frequently updated information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f833c4d-2f37-493d-b4f3-ef801d367cb9",
   "metadata": {},
   "source": [
    "## Intitializing the environment\n",
    "\n",
    "### Setting up the API key\n",
    "\n",
    "Before we start work on in any notebook, we'll need to load the [API key for Model Studio](https://modelstudio.console.alibabacloud.com/?tab=globalset#/efm/api_key). This ensures that we can call APIs of Qwen models we'll be using throughout this course. \n",
    "\n",
    "> If you're unsure about how to find your **Model Studio** API key, refer to the `00 Setting Up the Environment` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba1d3c-199e-4866-a991-f9dad0040845",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Model Studio API key\n",
    "import os\n",
    "from config.load_key import load_key\n",
    "load_key(\n",
    "    confirmation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011523dd-25da-477b-95cf-fd03a2612007",
   "metadata": {},
   "source": [
    "### Setting up the LLM and embedding model\n",
    "\n",
    "We set up Alibaba Cloud's `qwen-plus` as the LLM and DashScope's `text-embedding-v3` embedding model.\n",
    "\n",
    "For this lesson, we'll be using `OpenAILike` instead of `OpenAI`, which we were using before this. `OpenAILike` is a **LlamaIndex-specific wrapper** designed for OpenAI-compatible models, including:\n",
    "\n",
    "* Model Studio\n",
    "* Dashscope\n",
    "* vLLM\n",
    "* Ollama\n",
    "* Local LLMs with OpenAI-compatible APIs\n",
    "\n",
    "\n",
    "> **Note:** DashScope takes `https://dashscope-intl.aliyuncs.com/api/v1` as its API endpoint instead of the `https://dashscope-intl.aliyuncs.com/compatible-mode/v1` we've been using so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8661bb-55cd-487a-be54-cfa5ffc66bd7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set global settings\n",
    "import time\n",
    "import logging\n",
    "import dashscope\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from pathlib import Path\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Dashscope uses https://dashscope-intl.aliyuncs.com/api/v1 \n",
    "# instead of https://dashscope-intl.aliyuncs.com/compatible-mode/v1\n",
    "dashscope.base_http_api_url =\"https://dashscope-intl.aliyuncs.com/api/v1\"\n",
    "\n",
    "Settings.llm=OpenAILike(\n",
    "    model=\"qwen-plus\",\n",
    "    api_base=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    is_chat_model=True\n",
    ")\n",
    "\n",
    "Settings.embed_model = DashScopeEmbedding(\n",
    "    model_name=\"text-embedding-v3\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Global parameters set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd7ae2-21ef-43df-b4e7-a4c3c844f0d0",
   "metadata": {},
   "source": [
    "# Limitations of Standalone LLMs\n",
    "\n",
    "---\n",
    "\n",
    "Before diving into Retrieval-Augmented Generation (RAG), it‚Äôs important to understand the **inherent limitations of standalone large language models (LLMs)**. While LLMs are remarkably capable at generating fluent, coherent text, they are not omniscient or perfectly reliable. Their behavior is shaped entirely by pre-training data and prompt input‚Äîmeaning they lack dynamic access to new, private, or real-time information.\n",
    "\n",
    "Understanding these limitations helps us make informed decisions about how to enhance LLMs for real-world applications like **TaskFriend**.\n",
    "\n",
    "## Key limitations\n",
    "\n",
    "### Knowledge cutoffs\n",
    "\n",
    "Most LLMs are trained on static datasets with a fixed knowledge cutoff date. For example:\n",
    "\n",
    "* **Alibaba Cloud's Qwen3:** April 2025\n",
    "* **OpenAI's GPT 4.1:** June 2024\n",
    "* **Google's Gemini 2.5 Pro:** Jan 2025\n",
    "* **Anthropic's Claude 4 Opus:** March 2025\n",
    "\n",
    "This means they **cannot know about events, products, or research published after that date**.\n",
    "\n",
    "> **üìå Example:** Ask a base LLM, ‚ÄúWho won the 2025 UEFA Champions League?‚Äù ‚Äî it will either guess or invent an answer.\n",
    "\n",
    "Even if the model is powerful, its knowledge is frozen in time.\n",
    "\n",
    "### No access to private or internal data\n",
    "\n",
    "LLMs are not connected to your personal task list, company wiki, or internal CRM. Unless explicitly provided, they have **zero awareness** of:\n",
    "\n",
    "* Your calendar\n",
    "* Your project notes\n",
    "* Company policies\n",
    "* Customer records\n",
    "\n",
    "This makes them **useless for personalized or enterprise tasks** without augmentation.\n",
    "\n",
    "> **üîê Security Note:** This isolation is actually a *feature* for privacy‚Äîbut it means we must *intentionally* connect them to data when needed.\n",
    "\n",
    "### Hallucinations\n",
    "When an LLM lacks sufficient information, it may **confidently generate false or fabricated content**‚Äîa phenomenon known as *hallucination*.\n",
    "\n",
    "> **üö® Example:**  \n",
    "> **User:** ‚ÄúWhat‚Äôs the deadline for the Q2 report?‚Äù  \n",
    "> **LLM:** ‚ÄúThe Q2 report is due on April 15.‚Äù  \n",
    "> **Reality:** No such report exists.\n",
    "\n",
    "This is dangerous in productivity, legal, medical, or customer-facing applications.\n",
    "\n",
    "## Alternatives and their drawbacks\n",
    "\n",
    "To overcome these limitations, several strategies exist‚Äîeach with tradeoffs in cost, scalability, and maintenance.\n",
    "\n",
    "| Approach          | Description                                  | Limitations |\n",
    "|-------------------|----------------------------------------------|-------------|\n",
    "| **Prompt Engineering** | Crafting prompts to guide behavior (e.g., system prompts, few-shot examples) | Limited by context window; static; fragile to input changes |\n",
    "| **Fine-tuning**   | Retraining the model on new data to internalize knowledge or style | Expensive; hard to update; risks overfitting; not versionable |\n",
    "| **Pure Retrieval** | Returning relevant documents or snippets without generation | Doesn‚Äôt synthesize answers; requires user to read; no natural language output |\n",
    "\n",
    "While each approach has its place, none offers a perfect balance of **accuracy, freshness, cost, and ease of maintenance**.\n",
    "\n",
    "\n",
    "## The spectrum of LLM enhancement: Context vs. model optimization\n",
    "\n",
    "There‚Äôs a fundamental tradeoff in how we enhance LLMs:\n",
    "\n",
    "| Axis | Description |\n",
    "|------|-------------|\n",
    "| **Model Optimization** | Changing the model itself (e.g., fine-tuning, distillation, pre-training) |\n",
    "| **Context Optimization** | Keeping the model fixed, but enriching the input context (e.g., RAG, prompt engineering, retrieval) |\n",
    "\n",
    "This leads to a strategic spectrum:  \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/LMP-C01_03-Model Engineering Matrix.gif\" style=\"max-width: 800px;\" />\n",
    "  <br>\n",
    "  <small>LLM optimization matrix</small>\n",
    "  <br>\n",
    "  <small><i>Source: <a href=\"https://platform.openai.com/docs/guides/optimizing-llm-accuracy\" target=\"_blank\">OpenAI - Optimizing LLM Accuracy</a></i></small>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4bcaa-df86-4ef1-be3a-c4564fcf4e91",
   "metadata": {},
   "source": [
    "# What is Retrieval Augmented Generation (RAG)?\n",
    "\n",
    "---\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a powerful technique that enhances large language models (LLMs) by integrating external knowledge into the generation process. In simple terms, RAG allows an AI model to look up relevant information from a knowledge base before generating a response. This makes the answers more accurate, up-to-date, and grounded in real-world data.\n",
    "\n",
    "RAG is a hybrid approach that combines two key components:\n",
    "\n",
    "* **Retrieval**: Finding the most relevant pieces of information from a large dataset.\n",
    "* **Generation**: Using a language model to craft a natural language response based on that retrieved information.\n",
    "\n",
    "This combination allows RAG to overcome some of the limitations of standalone LLMs, such as outdated knowledge or the tendency to hallucinate.\n",
    "\n",
    "## Why RAG matters\n",
    "\n",
    "Traditional LLMs are trained on massive datasets, but once deployed, their knowledge is static. They cannot access real-time or private data, which limits their usefulness in many applications. RAG solves this by allowing the model to dynamically pull in the most relevant information at the time of inference.\n",
    "\n",
    "| Problem | Without RAG | With RAG |\n",
    "|--------|-------------|---------|\n",
    "| ‚ÄúWhat tasks are due this week?‚Äù | Can‚Äôt answer (no access to data) | Retrieves actual task list |\n",
    "| ‚ÄúI need to break down my project‚Äù | General advice only | Uses real project notes |\n",
    "| ‚ÄúAre there any company policies on remote work?‚Äù | Might hallucinate | Pulls from HR docs |\n",
    "\n",
    "\n",
    "This makes RAG especially valuable in:\n",
    "* Enterprise environments (e.g., internal knowledge bases).\n",
    "* Research and academic settings (e.g., answering questions from scientific papers).\n",
    "* Customer support (e.g., answering queries using product documentation).\n",
    "\n",
    "With RAG, our LLM transforms from a *reactive chatbot* into a **proactive knowledge assistant**!\n",
    "\n",
    "\n",
    "## How RAG works: The core pipeline\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    classDef frameworkStyle fill:#ffffff,stroke:#1f77b4,stroke-width:2px;\n",
    "    \n",
    "    subgraph flowchart[RAG pipeline]\n",
    "        subgraph \"Your data\"\n",
    "            A[(Database)]\n",
    "            B[Document]\n",
    "            C[API]\n",
    "        end\n",
    "\n",
    "        U((User))\n",
    "        I[Index]\n",
    "        L[LLM]\n",
    "\n",
    "        A -- structured --> I\n",
    "        B -- unstructured --> I\n",
    "        C -- programmatic --> I\n",
    "\n",
    "        U -- query --> I\n",
    "        I -- \"prompt +<br>query +<br>relevant data\" --> L\n",
    "        L -- response --> U\n",
    "    end\n",
    "    \n",
    "    class flowchat frameworkStyle;\n",
    "```\n",
    "\n",
    "The RAG system operates in **three distinct stages**:\n",
    "\n",
    "\n",
    "### Stage 1: Retrieval\n",
    "\n",
    "When a user asks a question, the system first needs to find the most relevant pieces of information. This is done using a **retrieval model** that converts the query into a numerical representation (embedding) and searches a database of pre-embedded documents for the most similar matches.\n",
    "\n",
    "For example, if the user asks `‚ÄúWhat is the capital of France?‚Äù`, the system might retrieve a document that says `‚ÄúParis is the capital of France.‚Äù`\n",
    "\n",
    "### Stage 2: Augmentation\n",
    "\n",
    "Once the system retrieves relevant documents, it injects them directly into the prompt as **context**. This transforms a generic query into a data-rich instruction the LLM can act on.\n",
    "\n",
    "Example:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"User Query:<br>'What is on my shopping list?'\"] --> D((Prompt))\n",
    "    B[\"Retrieved Context:<br>'- Bread<br>- Milk<br>- Jam'\"] --> D\n",
    "    C[LLM] --> Response[\"Response:<br>'Your shopping list is:<br>bread, milk, jam'\"]\n",
    "    \n",
    "    subgraph \"Augmented Prompt\"\n",
    "        D --> E[\"'Based on the following:<br>- Bread<br>- Milk<br>- Jam<br>Answer: What tasks are due today?'\"]\n",
    "    end\n",
    "\n",
    "    E --> C\n",
    "```\n",
    "\n",
    "### Stage 3: Generation\n",
    "\n",
    "The LLM generates a natural language response **grounded in the retrieved data**, reducing hallucinations and increasing accuracy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f941b-3b6f-4215-ac00-b9ede54a964c",
   "metadata": {},
   "source": [
    "# Understanding Embeddings and Vector Search\n",
    "\n",
    "At the heart of RAG is vector search, powered by embeddings ‚Äî dense numerical representations of text.\n",
    "\n",
    "## What is an embedding?\n",
    "An embedding is a fixed-length vector (e.g., length `1024`) that represents the semantic meaning of a piece of text. Similar texts have similar embeddings.\n",
    "\n",
    "> üí° Analogy: Think of documents as books in a library. Embeddings are like GPS coordinates ‚Äî they help us find the closest matches.\n",
    "\n",
    "## How does vector search work?\n",
    "\n",
    "* The user query is converted into an embedding.\n",
    "* The system searches a vector index for the most similar embeddings.\n",
    "* The top matching documents are returned and used to augment the prompt.\n",
    "\n",
    "This process is powered by **cosine similarity** ‚Äî a way to measure how alike two pieces of text are in meaning, even if their words differ.\n",
    "\n",
    "\n",
    "**Why \"angle\" matters more than \"distance\"**\n",
    "\n",
    "Imagine each document (and the query) lives as a point in a high-dimensional space ‚Äî say, `1024` dimensions. You can‚Äôt visualize that, but here‚Äôs the key idea:\n",
    "\n",
    "**Cosine similarity** looks at the angle between two vectors, not how far apart they are. \n",
    "\n",
    "* If two vectors point in nearly the same direction ‚Üí high similarity\n",
    "* If they point in opposite directions ‚Üí low similarity\n",
    "\n",
    "> **Pro tip:**  \n",
    "> Think of embeddings like arrows shot from the origin.  \n",
    "> Even if one arrow is longer (e.g., a longer document), what matters is where it‚Äôs aiming.  \n",
    "> Two arrows aiming in the same direction represent similar meanings ‚Äî and cosine similarity captures that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbde53f-6519-4d34-8a15-32f87aaef7ef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Use configured embedding model\n",
    "embed_model = Settings.embed_model\n",
    "\n",
    "# Step 2: Sample documents\n",
    "docs = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"The Eiffel Tower is in Paris.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Tokyo is the capital of Japan.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\"\n",
    "]\n",
    "\n",
    "# Step 3: Import and plot\n",
    "from functions.vector_visualization import plot_vector_search\n",
    "\n",
    "query = \"What is the capital city of France?\"\n",
    "\n",
    "plot_vector_search(embed_model, docs, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d08c8-10d2-4f52-9991-1c9fef87e946",
   "metadata": {},
   "source": [
    "# Building the RAG System Step-by-Step\n",
    "\n",
    "Let‚Äôs walk through how to build a working RAG system using `llama_index`, `DashScope`, and our local `documents`.\n",
    "\n",
    "## Step 1: Load the Documents\n",
    "\n",
    "LlamaIndex provides the `SimpleDirectoryReader`, which we will use to load files from the `./docs/taskfriend` directory.\n",
    "\n",
    "> **Note:** files may be separted into multiple pieces by `SimpleDirectoryReader`.  \n",
    "> For our example, the embedder takes a maximum of 10 pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355a601-84a5-4bf2-a642-7e84ea2b25c4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"./docs/taskfriend\",\n",
    "    required_exts=[\".pdf\"],\n",
    "    recursive=False\n",
    ").load_data()\n",
    "\n",
    "print(f\"\\nüìÑ Raw documents loaded: {len(documents)}\")\n",
    "for doc in documents:\n",
    "    print(f\" - {Path(doc.metadata['file_path']).name} (Text len: {len(doc.text)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6de517-334c-4cba-9741-eac8a68e6bea",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-01T16:23:11.497131Z",
     "iopub.status.busy": "2025-08-01T16:23:11.496853Z",
     "iopub.status.idle": "2025-08-01T16:23:11.502856Z",
     "shell.execute_reply": "2025-08-01T16:23:11.502209Z",
     "shell.execute_reply.started": "2025-08-01T16:23:11.497111Z"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Build and Save the Index\n",
    "\n",
    "Next, we use LlamaIndex's `VectorStoreIndex.from_documents()` function to build a vector index from the documents we loaded, and persist it to disk. \n",
    "\n",
    "> **Pro tip:** Persisting to disk helps improve the speed of our RAG since we don't need to rebuild the index every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279fe30-09d8-4365-b209-56f328df521f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build index from documents\n",
    "print(\"Creating index...\", end=\"\", flush=True)\n",
    "start_time = time.time()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\" Done ‚úì ({load_time:.1f} seconds)\")\n",
    "\n",
    "# Save index\n",
    "index.storage_context.persist(\"knowledge_base/taskfriend\")\n",
    "print(\"‚úÖ Index built and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14077c5c-6bb8-4889-aa9e-d73cce6308dd",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"./docs/taskfriend\",\n",
    "    required_exts=[\".pdf\"],\n",
    "    recursive=False\n",
    ").load_data()\n",
    "\n",
    "print(\"Raw chunks:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n\")\n",
    "    print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5eeb66-52c2-418e-b6a5-e6fe64d3f0c5",
   "metadata": {},
   "source": [
    "## Step 3: Query the RAG System\n",
    "\n",
    "Now, use the `index.as_query_engine()` function to create the `query_engine`. Then, we'll build a wrapper for multi-turn conversations call it from our **TaskFriend** app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac26b85-be8c-4d85-aa0c-003e1add29ee",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from taskfriend.chat import chat_interface, wrap_rag_for_chat\n",
    "\n",
    "# Build the query engine (used to implement RAG)\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True,\n",
    "    llm=Settings.llm,\n",
    ")\n",
    "\n",
    "# üìù Define & initialize full_conversation\n",
    "full_conversation = []\n",
    "\n",
    "def get_rag_response(question, query_engine):\n",
    "    \n",
    "    try:\n",
    "        # üîç Query the RAG engine\n",
    "        response = query_engine.query(question)\n",
    "\n",
    "        # üß† Extract the answer\n",
    "        if hasattr(response, 'response'):\n",
    "            answer = response.response\n",
    "        else:\n",
    "            answer = str(response)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[RAG Error] {e}\")\n",
    "        return \"[Error retrieving response]\"\n",
    "\n",
    "\n",
    "# Wrap function for compatibility\n",
    "wrapped_rag = wrap_rag_for_chat(\n",
    "    get_rag_response,\n",
    "    query_engine=query_engine,\n",
    ")\n",
    "\n",
    "# Start chat with RAG\n",
    "chat_interface(\n",
    "    full_conversation=full_conversation,\n",
    "    # client=client,\n",
    "    call_llm_fn=wrapped_rag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f466e-1983-48b9-bb53-fe04bcf952ca",
   "metadata": {},
   "source": [
    "Now, try asking your model the following questions:\n",
    "\n",
    "```\n",
    "\"What tasks are due today?\"\n",
    "\"What tasks are due this week?\"\n",
    "```\n",
    "\n",
    "Congratulations! You've successfully created your first RAG!  \n",
    "The model can now read from the `tasks.pdf` file in `./docs/taskfriend`, giving you answers about the tasks you have. Here's a table of the tasks (if you can't find `tasks.pdf`):\n",
    "\n",
    "\n",
    "| ID | Task | Type | Due | Status | Notes |\n",
    "|----|------|------|-----|--------|-------|\n",
    "| 01 | Finalize Q3 OKRs by 3pm | One-off | Today | Pending | Collaborate with department heads to align on measurable objectives, lay out solid plan to achieve objectives and assign responsibility to team members. |\n",
    "| 02 | Prepare presentation for client review | One-off | This Week | Pending | Focus on deliverables from Q2, highlight success metrics, and outline next steps. Obtain client feedback on presentation and tweak direction based on client preferences. |\n",
    "| 03 | Onboard new team member | One-off | Today | Done | Schedule intro meetings with team members, send welcome email with onboarding checklist, assign mentor for first 30 days.<br>Karen was assigned to be the mentor for the new team member. |\n",
    "| 04 | Review team feedback survey results | One-off | This Week | Pending | Analyze anonymous feedback from recent engagement survey and identify top 3 pain points and 2 strengths. |\n",
    "| 05 | Update Project Phoenix roadmap | One-off | Today | Pending | Sync with project leads to reflect latest timelines, milestones, and resource allocations, taking into account the latest changes to supply-chain disruptions. |\n",
    "| 06 | Schedule 1:1s with team | One-off | This Month | Pending | Book 30-minute slots via calendar invite with each team member over the next 4 weeks, focus agenda on career paths, workload balance, and feedback. |\n",
    "| 07 | Call bank regarding home loan | One-off | This Week | Pending | Contact customer service to inquire about refinancing options, and compare current interest rate with market rates, inquire about early repayment penalties and eligibility for better terms based on current market conditions. |\n",
    "| 08 | Weekly report: Project Phoenix | Recurring | This Week | Started | Compile progress on deliverables, blockers, and resource usage, share report with stakeholders via DingTalk every Friday EOD. |\n",
    "| 09 | Develop 3-year plan | One-off | This Year | Pending | Based on company strategy shifts and market trends, draft a long-term vision for the team, and present draft at annual planning retreat. |\n",
    "| 10 | Write thank-you letter to penpal in Korea | One-off | Today | Started | Thank penpal in Korea for the help they provided when you needed advice on planning a trip to Norway, and remember to ask them about their newborn son, Edwin. |\n",
    "\n",
    "\n",
    "However, you'll notice that your RAG is not perfect - the answer it gave you isn't representative of all the tasks you have. And as you continue to talk to **TaskFriend**, you'll realize that there are some questions it still can't answer correctly. We'll cover this in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b6411-02b8-4519-916e-869d22f7c103",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "\n",
    "## Quiz yourself!\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
    "<b>1. Which of the following is a key limitation of standalone LLMs that RAG helps solve?</b>  \n",
    "\n",
    "<ul>\n",
    "    <li>A) High API costs  </li>\n",
    "    <li>B) Inability to generate fluent text  </li>\n",
    "    <li>C) Lack of access to private or real-time data  </li>\n",
    "    <li>D) Slow inference speed</li>\n",
    "</ul>\n",
    "\n",
    "**View answer ‚Üí**\n",
    "</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
    "\n",
    "‚úÖ **Correct answer:** C) Lack of access to private or real-time data  \n",
    "üìù **Explaination**Ôºö\n",
    "* RAG enables LLMs to retrieve and use up-to-date, user-specific information (e.g., tasks, calendars).\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
    "<b>2. In the RAG pipeline, what happens during the \"Augmentation\" stage?</b>  \n",
    "\n",
    "<ul>\n",
    "    <li>A) Embeddings are retrained  </li>\n",
    "    <li>B) The model fine-tunes on new documents  </li>\n",
    "    <li>C) The user is shown raw search results  </li>\n",
    "    <li>D) Retrieved documents are added to the prompt as context</li>\n",
    "</ul>\n",
    "\n",
    "**View answer ‚Üí**\n",
    "</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
    "\n",
    "‚úÖ **Correct answer:** D) Retrieved documents are added to the prompt as context  \n",
    "üìù **Explaination**Ôºö\n",
    "* This allows the LLM to generate responses grounded in actual data.\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "## Takeaways\n",
    "\n",
    "* **Limitations of standalone LLMs**\n",
    "    * **Knowledge cutoffs** mean LLMs cannot know about events or data after their training date (e.g., Qwen3: April 2025).\n",
    "    * **No access to private or internal data** ‚Äî LLMs don‚Äôt see your calendar, tasks, or company docs unless explicitly provided.\n",
    "    * **Hallucinations** occur when models lack information and invent plausible-sounding but false answers.\n",
    "    * **Alternatives have tradeoffs**:\n",
    "      * *Prompt engineering*: Limited by context window.\n",
    "      * *Fine-tuning*: Expensive, hard to update.\n",
    "      * *Pure retrieval*: Returns raw text, no natural language synthesis.\n",
    "    * **RAG solves these** by dynamically injecting real, up-to-date, user-specific context at inference time.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **RAG systems**\n",
    "    * **RAG bridges the gap** between general knowledge and specific, private, or real-time data.\n",
    "    * It combines two powerful components:\n",
    "      - **Retrieval**: Find relevant documents from a knowledge base.\n",
    "      - **Generation**: Use an LLM to generate a natural language response based on retrieved content.\n",
    "    * **RAG transforms LLMs** from static chatbots into dynamic knowledge assistants.\n",
    "    * It enables accurate, personalized responses to questions like:\n",
    "      - ‚ÄúWhat tasks are due this week?‚Äù\n",
    "      - ‚ÄúCan I reschedule my presentation prep?‚Äù\n",
    "    * **RAG is context optimization**, not model optimization ‚Äî the LLM stays fixed, but the input is enriched.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Embeddings and vector search**\n",
    "    * **Embeddings** are dense numerical vectors (e.g., length 1024) that represent semantic meaning.\n",
    "    * **Similar texts have similar embeddings** ‚Äî this enables semantic search beyond keyword matching.\n",
    "    * **Vector search** finds the most relevant documents by comparing query and document embeddings.\n",
    "    * **Cosine similarity** measures semantic alignment by the *angle* between vectors, not distance:\n",
    "      - Small angle ‚Üí high similarity\n",
    "      - Opposite directions ‚Üí low similarity\n",
    "    * **Embeddings allow the system to understand** that ‚Äúcapital of France‚Äù and ‚ÄúParis‚Äù are related, even if the words don‚Äôt match exactly.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Building a RAG system**\n",
    "    * **Step 1: Load documents** using tools like `SimpleDirectoryReader` to ingest PDFs, text files, or APIs.\n",
    "    * **Step 2: Build a vector index** ‚Äî convert documents into embeddings and store them for fast retrieval.\n",
    "    * **Step 3: Persist the index** to disk so it doesn‚Äôt need to be rebuilt every time.\n",
    "    * **Step 4: Query with augmentation** ‚Äî retrieve relevant context and inject it into the prompt.\n",
    "    * **The RAG pipeline**:\n",
    "      1. **Retrieval**: Convert query to embedding ‚Üí find top-matching documents.\n",
    "      2. **Augmentation**: Add retrieved content to the prompt as context.\n",
    "      3. **Generation**: LLM generates a grounded, accurate response.\n",
    "    * **RAG is iterative** ‚Äî your first version may not be perfect, but it‚Äôs a foundation for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineer (Professional)",
   "language": "python",
   "name": "llm_pro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
