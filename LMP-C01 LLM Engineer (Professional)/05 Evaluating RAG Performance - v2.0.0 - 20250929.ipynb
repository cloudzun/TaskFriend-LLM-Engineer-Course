{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030dace5-6096-4fc3-a6a4-eea583c224e1",
   "metadata": {},
   "source": [
    "# 评估 RAG 性能\n",
    "\n",
    "---\n",
    "\n",
    "祝贺你构建了第一个 RAG！RAG（Retrieval-Augmented Generation，检索增强生成）是打造实用型 LLM 应用最常见的方式之一。它让你能够引入外部知识，使你的 AI 助手或聊天机器人不再局限于模型在训练阶段记住的内容。\n",
    "\n",
    "但加入检索功能也让整体复杂度显著提升。\n",
    "\n",
    "此时你不再只是给 LLM 写提示词——你在搭建整条管线。这里涉及文本分块、检索、上下文拼装与生成。当出现问题时，根源往往不明：是模型幻觉？还是压根没有拿到正确的信息？\n",
    "\n",
    "那么，在向用户发布之前，如何判断 RAG 系统的表现如何呢？这正是评估机制发挥作用的地方。\n",
    "\n",
    "## 故事回顾\n",
    "\n",
    "你已经成功把 RAG 整合进 **TaskFriend**，现在它可以访问应用中存放的其他信息。然而，你隐约觉得某处出了问题，于是对比了应用数据与 **TaskFriend** 的回答。果不其然——**TaskFriend** 会把部分信息弄错。你担心这可能只是冰山一角，可是完全靠人工排查显然不可行。\n",
    "\n",
    "因此，你决定构建一个评估机制，帮助你自动化衡量 **TaskFriend** 的 RAG 功能表现如何。\n",
    "\n",
    "## 目标\n",
    "\n",
    "* 了解为什么人工评估适合调试，却无法在规模化场景下独立支撑。\n",
    "* 学会逐步检查 RAG 管线，以定位故障点。\n",
    "* 掌握 RAG 评估的核心维度：上下文召回、上下文精度、事实可信度与答案正确性。\n",
    "* 搭建并使用 Ragas 来自动评估 RAG 系统。\n",
    "* 解读评估指标，提炼可执行洞见，从而改进 RAG 管线。\n",
    "* 比较常见的 RAG 评估框架，理解各自的适用场景。\n",
    "* 建立可持续、专家参与的评估最佳实践流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4293289-3769-44ea-afef-78fcb20a7af0",
   "metadata": {},
   "source": [
    "## 初始化环境\n",
    "\n",
    "### 设置 API 密钥\n",
    "\n",
    "在开始任何笔记本的工作之前，我们需要加载 [Model Studio 的 API 密钥](https://modelstudio.console.alibabacloud.com/?tab=globalset#/efm/api_key)。这样才能调用我们在整个课程中使用的 Qwen 模型接口。\n",
    "\n",
    "> 如果你不确定如何找到 **Model Studio** 的 API 密钥，请参考 `00 Setting Up the Environment` 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f10a24-a058-4759-8bc0-69a6d59e02f0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Model Studio API key\n",
    "import os\n",
    "from config.load_key import load_key\n",
    "load_key(\n",
    "    confirmation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0b040-078c-4f02-8827-6176ad2d8aaa",
   "metadata": {},
   "source": [
    "### 配置 LLM 与嵌入模型\n",
    "\n",
    "与上一课相同，我们将使用阿里云的 `qwen-plus` 作为 LLM，并使用 DashScope 的 `text-embedding-v3` 作为嵌入模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda9fb2-bdb2-4ad0-94a1-f340c394609d",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set global settings\n",
    "import time\n",
    "import logging\n",
    "import dashscope\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from pathlib import Path\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Route DashScope calls to the mainland China endpoints\n",
    "dashscope.base_http_api_url = \"https://dashscope.aliyuncs.com/api/v1\"\n",
    "\n",
    "Settings.llm = OpenAILike(\n",
    "    model=\"qwen-plus\",\n",
    "    api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    is_chat_model=True\n",
    ")\n",
    "\n",
    "Settings.embed_model = DashScopeEmbedding(\n",
    "    model_name=\"text-embedding-v3\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "print(\"✅ Global parameters set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db2223-83ea-46c1-8942-489f43722c6e",
   "metadata": {},
   "source": [
    "# RAG 评估流程\n",
    "\n",
    "--- \n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[手动评估] --> B[手动排障] --> C[自动化评估]\n",
    "```\n",
    "\n",
    "在真实生产环境中，由于需要处理的数据量巨大，评估 RAG 系统通常最好交给**自动化评估**方法。不过，今天我们聚焦在较小的范围——我们新搭建的 **TaskFriend** 应用。\n",
    "\n",
    "## 手动评估：找出问题\n",
    "\n",
    "在先前的示例中，你或许记得出现过类似的对话：\n",
    "\n",
    "```\n",
    "🚀 TaskFriend Conversation\n",
    "------------------------------------------------------------\n",
    "👤 You: How many tasks do I need to complete today?\n",
    "\n",
    "🤖 TaskFriend is thinking...\n",
    "🤖 TaskFriend: You have one task that is due today.\n",
    "\n",
    "------------------------------------------------------------\n",
    "```\n",
    "\n",
    "然而，当我们查看 `tasks.pdf` 时，却发现事实并非如此：\n",
    "\n",
    "\n",
    "| ID | 任务 | 类型 | 截止 | 状态 | 备注 |\n",
    "|----|------|------|-----|--------|-------|\n",
    "| 01 | 下午 3 点前完成 Q3 OKR | 一次性 | ⭐今天 | 待处理 | 与各部门负责人协作以对齐可量化目标，制定实现目标的可靠计划，并为团队成员分配职责。 |\n",
    "| 03 | 新成员入职安排 | 一次性 | ⭐今天 | 已完成 | 安排与团队成员的介绍会议，发送包含入职清单的欢迎邮件，为前 30 天指定导师。<br>Karen 已被指派为新成员的导师。 |\n",
    "| 05 | 更新凤凰项目路线图 | 一次性 | ⭐今天 | 待处理 | 与项目负责人同步以反映最新时间线、里程碑和资源分配，并考虑供应链中断的最新变化。 |\n",
    "| 10 | 给韩国笔友写感谢信 | 一次性 | ⭐今天 | 进行中 | 感谢在规划挪威旅行时提供帮助的韩国笔友，并记得询问他们新出生的儿子 Edwin 的近况。 |\n",
    "\n",
    "\n",
    "那么发生了什么？这个问题的根源是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e1437-3e9f-437e-ad88-d47c3c7311c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 排查你的 RAG：上手实践\n",
    "\n",
    "让我们再运行一次由 RAG 驱动的 **TaskFriend**。但这次，我们会同时**打印 RAG 从索引中检索到的文本块（或节点）**。\n",
    "\n",
    "另外，我们还会添加一个自定义的 `highlight_words` 函数，以便更清晰地观察输出。稍后你就会明白它的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a42ec-710c-4e6a-81b2-5bc06a0228d1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highlight_words(text, words_to_highlight, emoji=\"⭐\"):\n",
    "    for word in words_to_highlight:\n",
    "        if word in text:\n",
    "            text = text.replace(word, f\"{emoji}{word}{emoji}\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_rag_response_with_info(query, query_engine, highlight=None):\n",
    "    print(\"🚀 TaskFriend Conversation (single-round)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f'👤 You: {query}')\n",
    "    \n",
    "    try:\n",
    "        # 🔍 Query the RAG engine\n",
    "        response = query_engine.query(query)\n",
    "\n",
    "        # 🧠 Extract the answer\n",
    "        if hasattr(response, 'response'):\n",
    "            answer = response.response\n",
    "        else:\n",
    "            answer = str(response)\n",
    "\n",
    "        # 📚 Show source references AFTER the answer\n",
    "        print(\"🤖 TaskFriend:\", answer)\n",
    "        print('\\n\\n' + '=' * 50)\n",
    "        print('📚 References\\n')\n",
    "        \n",
    "        highlight = highlight or []\n",
    "        for i, source_node in enumerate(response.source_nodes, start=1):\n",
    "            print(f'Chunk {i}:')\n",
    "            \n",
    "            # Highlight words in chunk\n",
    "            highlighted_text = highlight_words(source_node.text, highlight)\n",
    "            print(highlighted_text)\n",
    "            print()\n",
    "        print('=' * 50)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[RAG Error] {e}\")\n",
    "        return \"[Error retrieving response]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f4246-8b9c-4daf-a75f-bc321c1d9986",
   "metadata": {},
   "source": [
    "由于我们在上一章节已经构建并保存了索引，因此现在只需要从保存的位置（`./knowledge_base/taskfriend`）加载即可。\n",
    "\n",
    "这里我们会用到 `llama_index` 中的一些函数：\n",
    "\n",
    "* `StorageContext:` 存储上一章保存的节点、嵌入与向量库，使你的代码能够定位并连接这些数据，这样就无需重新处理所有文档，就能重新加载整个知识库。\n",
    "* `load_index_from_storage:` 根据保存在 `StorageContext` 中的文件重建 RAG 索引。这是让 RAG 系统快速再次运行的最高效方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782736b2-6306-4109-9271-43a3c86d190a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "persist_path=\"./knowledge_base/taskfriend\"\n",
    "\n",
    "# Import index (\"knowledgebase\") we built last chapter,\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=persist_path,\n",
    ")\n",
    "\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    embed_model=Settings.embed_model\n",
    ")\n",
    "print(f\"✅ Index loaded from `{persist_path}`!\")\n",
    "\n",
    "# Build the query engine (used to implement RAG)\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=False,\n",
    "    llm=Settings.llm,\n",
    ")\n",
    "print(\"✅ Query engine built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4484f35-73c4-48c2-8476-488dd6de0937",
   "metadata": {},
   "source": [
    "现在我们就可以再次运行同样的查询，只不过这一次会同时查看 **TaskFriend** 获取信息的来源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56694613-71e5-4c58-8aa7-6a72078053fc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User query\n",
    "query = \"How many tasks do I need to complete today?\"\n",
    "\n",
    "# Choose words to highlight\n",
    "highlight = [\"Today\"]\n",
    "\n",
    "response = get_rag_response_with_info(query, query_engine=query_engine, highlight=highlight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2050137-82b7-4672-804a-a840a62478b0",
   "metadata": {},
   "source": [
    "现在我们会看到类似这样的输出：\n",
    "\n",
    "```\n",
    "🚀 TaskFriend Conversation (single-round)\n",
    "--------------------------------------------------\n",
    "👤 You: How many tasks do I need to complete today?\n",
    "🤖 TaskFriend: You need to complete two tasks today. One is to finalize Q3 OKRs by 3 pm, and the other is to onboard a new team member, which has already been marked as done.\n",
    "\n",
    "\n",
    "==================================================\n",
    "📚 References\n",
    "\n",
    "Chunk 1:\n",
    "Tasks   ID Task Type Due Status Notes \n",
    "01 Finalize Q3 OKRs by 3pm One-oﬀ ⭐Today⭐ Pending  Collaborate with department heads to align on measurable objecDves, lay out solid plan to achieve objecDves and assign responsibility to team members.   \n",
    "02 Prepare presentaDon for client review One-oﬀ This Week Pending  Focus on deliverables from Q2, highlight success metrics, and outline next steps. Obtain client feedback on presentaDon and tweak direcDon based on\n",
    "\n",
    "Chunk 2:\n",
    "client preferences.   \n",
    "03 Onboard new team member One-oﬀ ⭐Today⭐ Done  Schedule intro meeDngs with team members, send welcome email with onboarding checklist, assign mentor for ﬁrst 30 days.  Karen was assigned to be the mentor for the new team member.   04 Review team feedback survey results One-oﬀ This Week Pending  Analyze anonymous feedback from recent engagement survey and idenDfy top 3 pain points and 2 strengths.\n",
    "\n",
    "==================================================\n",
    "```\n",
    "\n",
    "几件事情会格外显眼：\n",
    "\n",
    "* 单词 `Today` 只出现了两次。（感谢 `highlight_words` 帮我们突显！）\n",
    "* RAG 只检索了 *2 个文本块*。\n",
    "* 解析器似乎出现了问题，特别是一些拼写：\n",
    "    * Executive → ExecuHve\n",
    "    * meeDngs → meetings\n",
    "    \n",
    "基于这些线索，你已经成功确定了 RAG 系统问题的根源。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d918713-212f-4734-bb6c-0ab05536869b",
   "metadata": {},
   "source": [
    "# 自动化评估\n",
    "\n",
    "---\n",
    "\n",
    "对于一次性场景或像 TaskFriend 这种早期开发阶段的应用来说，手动评估与排障完全没有问题——深入探究开发细节也能帮助你快速成长。但一旦规模上来，靠人工逐条排查问题就越来越不可行了。\n",
    "\n",
    "接下来我们看看如何让这个流程实现自动化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324412bd-ce78-4fac-ba92-73ba40c2641f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_dashscope import DashScopeEmbeddings \n",
    "import os\n",
    "import time\n",
    "\n",
    "dashscope.base_http_api_url = \"https://dashscope.aliyuncs.com/api/v1\"\n",
    "\n",
    "# LangChain LLM for Ragas\n",
    "ragas_llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\",\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    timeout=60,\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "# LangChain Embeddings for Ragas\n",
    "ragas_embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v3\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"✅ LLM and Embeddings pipeline for Ragas successfully built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465479c0-5e3a-471d-ae71-a78ced56eeca",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new query engine for evaluation\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=ragas_llm,\n",
    "    embed_model=Settings.embed_model,\n",
    "    streaming=False  # Disable streaming for evaluation\n",
    ")\n",
    "\n",
    "print(\"✅ Query engine built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314e789-7cd1-420c-9ff6-e889a713906d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"How many tasks do I need to complete today?\",\n",
    "        \"ground_truth\": (\n",
    "            \"You have 3 tasks to complete today: Finalize Q3 OKRs, \"\n",
    "            \"Update Project Phoenix roadmap, and Write thank-you letter to penpal.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Am I planning a trip somewhere?\",\n",
    "        \"ground_truth\": \"You are planning a trip to Norway.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When is the 'Develop 3-year plan' task due?\",\n",
    "        \"ground_truth\": \"This year\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the one-time and recurring tasks for 'Project Phoenix'? Be brief.\",\n",
    "        \"ground_truth\": (\n",
    "            \"One-time tasks: Updating the project roadmap; \"\n",
    "            \"Recurring tasks: Generating weekly reports.\"\n",
    "        )       \n",
    "    },    \n",
    "]\n",
    "\n",
    "print(\"✅ Test cases defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ea654-b380-43c2-bfbd-fc4092cfc8a8",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functions.html_table import create_html_table\n",
    "\n",
    "# Define function to run test cases with evaluator LLM\n",
    "def run_test_cases(query_engine, test_cases):\n",
    "    results = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [],  # Store raw contexts for Dataset\n",
    "        \"contexts_display\": [],  # Store formatted contexts for display\n",
    "        \"ground_truth\": []\n",
    "    }\n",
    "    \n",
    "    print(\"Generating answers based on test cases...\", end=\"\", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for case in test_cases:\n",
    "        # Get response\n",
    "        response = query_engine.query(case[\"question\"])\n",
    "        \n",
    "        # Extract answer and context\n",
    "        answer = str(response).strip()\n",
    "        # Ensure contexts is a list of strings\n",
    "        contexts = [node.get_content().strip() for node in response.source_nodes]\n",
    "        \n",
    "        # Format contexts for display with [] and line breaks\n",
    "        contexts_display = [f\"[{ctx}]\" for ctx in contexts]\n",
    "        \n",
    "        # Store results\n",
    "        results[\"question\"].append(case[\"question\"])\n",
    "        results[\"answer\"].append(answer)\n",
    "        results[\"contexts\"].append(contexts)\n",
    "        results[\"contexts_display\"].append(\"⭐\".join(contexts_display)) # Use this to better visualize breaks between contexts\n",
    "        results[\"ground_truth\"].append(case[\"ground_truth\"])\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\" ✅ Done ({load_time:.1f} seconds)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "test_results = run_test_cases(query_engine, test_cases)\n",
    "\n",
    "table_data = {\n",
    "    \"question\": test_results[\"question\"],\n",
    "    \"answer\": test_results[\"answer\"],\n",
    "    \"ground_truth\": test_results[\"ground_truth\"],\n",
    "    \"contexts\": test_results[\"contexts_display\"],\n",
    "}\n",
    "\n",
    "# Use 5:15:15:15:50 percentage distribution (index + 4 columns)\n",
    "create_html_table(\n",
    "    table_data, \n",
    "    title=\"RAG Evaluation Results\", \n",
    "    column_widths=[5, 15, 15, 15, 50]  # index:question:contexts:ground_truth:answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3628f-88bb-4d80-a11c-33a0fb8e5cd7",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert to Dataset\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"question\": test_results[\"question\"],\n",
    "    \"answer\": test_results[\"answer\"],\n",
    "    \"contexts\": test_results[\"contexts\"],\n",
    "    \"ground_truth\": test_results[\"ground_truth\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b3c4d-f120-40e7-9b49-6a4809737cf0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "    faithfulness\n",
    ")\n",
    "from functions.rag_eval_table import create_rag_evaluation_table\n",
    "\n",
    "# Use the LangChain-compatible LLM for Ragas\n",
    "results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "        answer_correctness,\n",
    "        faithfulness\n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings\n",
    ")\n",
    "\n",
    "# Convert results to pandas DataFrame\n",
    "results_df = results.to_pandas()\n",
    "\n",
    "# Extract the input fields from the dataset to add context\n",
    "results_df[\"question\"] = eval_dataset[\"question\"]\n",
    "results_df[\"ground_truth\"] = eval_dataset[\"ground_truth\"]\n",
    "results_df[\"answer\"] = eval_dataset[\"answer\"]\n",
    "\n",
    "# Reorder columns for better readability\n",
    "results_df = results_df[[\n",
    "    \"question\", \n",
    "    \"answer\", \n",
    "    \"ground_truth\", \n",
    "    \"answer_correctness\",\n",
    "    \"context_recall\", \n",
    "    \"context_precision\",\n",
    "    \"faithfulness\"\n",
    "]]\n",
    "\n",
    "# print(\"\\n📈 Evaluation Results:\")\n",
    "# print(results_df)\n",
    "create_rag_evaluation_table(results_df, show_contexts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdbe5e4-f430-4169-beb8-10cc9d73e55b",
   "metadata": {},
   "source": [
    "# 剖析 Ragas 评估指标\n",
    "\n",
    "---\n",
    "\n",
    "我们已经跑过评估并得到了分数。现在该掀开引擎盖，弄清楚每个指标到底意味着什么。就像医生会通过不同的血液检测来诊断病症一样，我们使用这些指标来诊断 RAG 管线的健康状况。\n",
    "\n",
    "记住，这些分数不只是总成绩——它们是一份诊断报告。`answer_correctness` 得分偏低说明“患者生病了”，而其他指标则告诉我们“病因何在”。\n",
    "\n",
    "> **注意：**  \n",
    "> 每个 Ragas 评估指标都有 LLM 版和非 LLM 版两种。Ragas 默认采用 LLM 驱动的评估指标，本节也将围绕这一套展开。  \n",
    "> 详情可参见 [Ragas 文档](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/)。\n",
    "\n",
    "下面我们逐一深入了解所使用的四个核心指标。\n",
    "\n",
    "## 上下文召回率（Context Recall）\n",
    "\n",
    "> 我们是否找到了正确的信息？\n",
    "\n",
    "**衡量内容：**  \n",
    "上下文召回率用于回答一个关键问题：我们的检索器是否找到了回答问题所需的全部信息？它衡量的是“参考答案中的相关事实，有多少比例出现在检索到的上下文中”。\n",
    "\n",
    "**重要性：**  \n",
    "这是构建可靠 RAG 系统的地基。如果检索不到正确的信息，LLM 就像在摸黑工作——即便模型再强，也无法凭空生成缺失的事实。召回得分高，说明检索阶段完成得很到位。\n",
    "\n",
    "Ragas 的做法是把 `ground_truth` 拆成一个个最小“陈述”，再检查每条陈述是否能在 `retrieved_contexts` 中找到依据。公式如下：\n",
    "\n",
    "$$\n",
    "    \\text{Context Recall} = \\frac{\\text{参考答案中被检索上下文支持的陈述数量}}{\\text{参考答案中的陈述总数}}\n",
    "$$\n",
    "\n",
    "### 直观示例\n",
    "\n",
    "我们来构造一个简单的测试用例，看看检索到的上下文如何直接影响召回得分。这里继续沿用之前设置好的 `ragas_llm` 与 `ragas_embeddings`。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b33ca2-7d5a-48b3-85fd-e2a000cd145a",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functions.metric_barchart import plot_metric_comparison\n",
    "\n",
    "# Define the test scenario\n",
    "question = \"How many tasks are due today?\"\n",
    "\n",
    "# The ground truth contains 4 distinct claims\n",
    "ground_truth = (\n",
    "    \"1. Finalize Q3 OKRs is due today, \"\n",
    "    \"2. Onboard new team member is due today, \"\n",
    "    \"3. Update project roadmap is due today, \"\n",
    "    \"4. Write thank-you letter is due today.\"\n",
    ")\n",
    "\n",
    "# Test Case 1: Perfect Retrieval - All 4 claims are present\n",
    "contexts_perfect = [\n",
    "    [\n",
    "        \"ID 01: Finalize Q3 OKRs - Due: Today\",\n",
    "        \"ID 03: Onboard new team member - Due: Today\",\n",
    "        \"ID 05: Update project roadmap - Due: Today\",\n",
    "        \"ID 14: Write thank-you letter - Due: Today\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Test Case 2: Partial Retrieval - Only 2 claims are present\n",
    "contexts_partial = [\n",
    "    [\n",
    "        \"ID 01: Finalize Q3 OKRs - Due: Today\",\n",
    "        \"ID 05: Update project roadmap - Due: Today\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Test Case 3: No Retrieval - 0 claims are present\n",
    "contexts_none = [\n",
    "    [\n",
    "        \"ID 02: Prepare presentation - Due: This Week\",\n",
    "        \"ID 08: Clean up email - Due: This Year\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create datasets\n",
    "data_perfect = {\"question\": [question], \"ground_truth\": [ground_truth], \"contexts\": contexts_perfect}\n",
    "data_partial = {\"question\": [question], \"ground_truth\": [ground_truth], \"contexts\": contexts_partial}\n",
    "data_none = {\"question\": [question], \"ground_truth\": [ground_truth], \"contexts\": contexts_none}\n",
    "\n",
    "dataset_perfect = Dataset.from_dict(data_perfect)\n",
    "dataset_partial = Dataset.from_dict(data_partial)\n",
    "dataset_none = Dataset.from_dict(data_none)\n",
    "\n",
    "# Evaluate all three cases\n",
    "result_perfect = evaluate(dataset_perfect, metrics=[context_recall], llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "result_partial = evaluate(dataset_partial, metrics=[context_recall], llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "result_none = evaluate(dataset_none, metrics=[context_recall], llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "\n",
    "# Extract scores\n",
    "scores = {\n",
    "    \"Perfect Retrieval\\n(4/4 claims)\": result_perfect[\"context_recall\"],\n",
    "    \"Partial Retrieval\\n(2/4 claims)\": result_partial[\"context_recall\"],\n",
    "    \"No Relevant Retrieval\\n(0/4 claims)\": result_none[\"context_recall\"]\n",
    "}\n",
    "\n",
    "# Generate comparison chart\n",
    "plot_metric_comparison(\n",
    "    scores_dict=scores,\n",
    "    title=\"Context Recall Score vs. Retrieved Information Quality\",\n",
    "    ylabel=\"Context Recall Score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7eab11-cdb4-4e9f-bcc1-256fb2df8e35",
   "metadata": {},
   "source": [
    "**图表解读：**  \n",
    "召回分数会随着检索到的相关信息量线性下降。这证明召回本质上反映的是检索的完整性。\n",
    "\n",
    "### 提升上下文召回率\n",
    "\n",
    "* **调整块大小（Chunk Size）：** 如果文本块太小，单条事实可能被拆散到多个块里。尝试将 `chunk_size` 从 128 增大到 512。\n",
    "* **使用块重叠（Chunk Overlap）：** 在文本切分器中加入 `chunk_overlap=100`，以保留边界处的上下文。\n",
    "* **优化嵌入模型：** 试试不同的嵌入模型。更符合你领域数据的模型会生成更有意义的向量表示。\n",
    "* **实施重排序（Re-ranking）：** 使用交叉编码器模型对初次检索结果进行重排，把最相关的文本块提升到前面。\n",
    "* **采用混合检索：** 将向量检索与关键词检索（如 BM25）结合，捕捉仅靠嵌入可能遗漏的文档。\n",
    "\n",
    "\n",
    "## 上下文精度（Context Precision）\n",
    "\n",
    "> 我们检索来了多少噪声？\n",
    "\n",
    "**衡量内容：**  \n",
    "上下文精度聚焦于一个关键问题：**我们检索到的所有文本块中，有多少真正与查询相关？**  \n",
    "它衡量的是检索阶段的信噪比。\n",
    "\n",
    "**重要性：**  \n",
    "如果精度得分很低，说明 LLM 正在被大量无关信息“投喂”。这不仅会分散模型注意力、推高处理成本，还可能让模型被“伪线索”误导而给出错误答案。\n",
    "\n",
    "Ragas 通过“LLM 裁判”来计算该指标。它不是简单的算术运算，而是调用一个独立的“评估器”LLM，综合分析 `question`、`ground_truth` 与 `retrieved_contexts`，评估检索到的文本块对问题的相关程度，以及最有用的信息是否排在列表前列。\n",
    "\n",
    "因此，这个分数反映的是对检索质量的细致判断，同时考虑了文本块的相关性与排序。\n",
    "\n",
    "$$\n",
    "    \\text{Precision@k} = {\\text{true positives@k} \\over  (\\text{true positives@k} + \\text{false positives@k})}\n",
    "$$\n",
    "\n",
    "\n",
    "### 直观示例\n",
    "\n",
    "我们来看看文本块的排列顺序如何影响 `context_precision` 分数。这里使用一个简单的查询和三组不同的检索结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc887b4-6ed2-4df2-8026-3a9a868163f8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the test scenario\n",
    "question = \"When is the project roadmap due?\"\n",
    "\n",
    "# Ground truth (based on your data, it's \"Project Leads\")\n",
    "ground_truth = \"The project roadmap is due today.\"\n",
    "\n",
    "# Test Case 1: High Precision\n",
    "contexts_high = [\n",
    "    [\n",
    "        \"Update project roadmap is due today\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Test Case 2: Med Precision\n",
    "contexts_med = [\n",
    "    [\n",
    "        \"Trip to Norway is in November\",\n",
    "        \"Update project roadmap is due today\",                   # Correct answer is preceded by one false answer\n",
    "        \"Write to penpal needs to be completed within 24 hours\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Test Case 3: Low Precision\n",
    "contexts_low = [\n",
    "    [\n",
    "        \"Trip to Norway is in November\",\n",
    "        \"Write to penpal needs to be completed within 24 hours\",\n",
    "        \"Update project roadmap is due today\"                    # Correct answer is preceded by two false answers\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "# Create and evaluate datasets\n",
    "def create_and_evaluate(contexts_list, question, ground_truth):\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"question\": [question],\n",
    "        \"ground_truth\": [ground_truth],\n",
    "        \"contexts\": contexts_list\n",
    "    })\n",
    "    result = evaluate(dataset, metrics=[context_precision], llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "    return result[\"context_precision\"]\n",
    "\n",
    "scores = {\n",
    "    \"High Precision\\n(Correct answer at top)\": create_and_evaluate(contexts_high, question, ground_truth),\n",
    "    \"Med Precision\\n(Correct answer buried once)\": create_and_evaluate(contexts_med, question, ground_truth),\n",
    "    \"Low Precision\\n(Correct answer buried twice)\": create_and_evaluate(contexts_low, question, ground_truth)\n",
    "}\n",
    "\n",
    "# Generate comparison chart\n",
    "plot_metric_comparison(\n",
    "    scores_dict=scores,\n",
    "    title=\"Context Precision Score: When Answers are Buried\",\n",
    "    ylabel=\"Context Precision Score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2b424-b0a4-434b-8299-634a8f668454",
   "metadata": {},
   "source": [
    "**图表解读：**  \n",
    "\n",
    "* **高分（1.000）：** 评估器 LLM 认为结果完美。唯一检索到的块高度相关，而且排在最前。\n",
    "* **中等（0.500）：** 评估器 LLM 发现正确信息虽然存在，但前面夹着一个无关块。这类“噪声”会被惩罚，从而显著拉低得分。\n",
    "* **低分（0.333）：** 评估器 LLM 看到正确信息被两个无关块掩盖。排序质量低，说明噪声很多。\n",
    "\n",
    "这个实验证明 `context_precision` 不仅关乎“检索到什么”，也关乎“如何排序”。高精度得分意味着检索系统既能找到正确信息，也能以 LLM 易于利用的顺序呈现。\n",
    "\n",
    "### 提升上下文精度\n",
    "\n",
    "* **调整相似度阈值：** 筛掉相似度低于某阈值的文本块。\n",
    "* **使用查询重写：** 例如采用 HyDE（假想文档嵌入）等方法，在检索前生成更优的查询。\n",
    "* **利用元数据过滤：** 如果数据附带元数据（如 task_type、priority），可在向量检索前据此缩小搜索范围。\n",
    "* **实现重排序：** 借助重排序器（re-ranker），把向量上接近但无关的文本块压到后面。\n",
    "\n",
    "优化上下文精度，能让 LLM 面对的是一组干净、聚焦且排序合理的信息，从而最大化产出准确且有用回答的概率。\n",
    "\n",
    "\n",
    "## 忠实度（Faithfulness）\n",
    "\n",
    "**衡量内容：**  \n",
    "忠实度关注的问题是：*LLM 是否出现幻觉？回答是否严格依据提供的上下文？*  \n",
    "它衡量的是生成答案中有多少陈述能够被检索到的上下文直接支持。\n",
    "\n",
    "**重要性：**  \n",
    "这是构建可信 AI 的关键。忠实度高意味着 LLM 不在凭空捏造内容，而是基于证据给出回答。\n",
    "\n",
    "计算方法与召回类似，但方向相反：\n",
    "\n",
    "$$\n",
    "    \\text{Faithfulness Score} = \\frac{\\text{回答中得到检索上下文支持的陈述数量}}{\\text{回答中的陈述总数}}\n",
    "$$\n",
    "\n",
    "### 直观示例\n",
    "我们来构造一个包含幻觉的回答。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc290e-3524-4de2-a1c1-bc537c081666",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the test case\n",
    "question = \"When is the 'Develop 3-year plan' task due?\"\n",
    "\n",
    "# The ground truth\n",
    "ground_truth = \"This Year\"\n",
    "\n",
    "# The retrieved context (our source of truth)\n",
    "contexts = [\n",
    "    [\n",
    "        \"The 'Develop 3-year plan' task is due 'This Year'.\",\n",
    "        \"The 'Finalize Q3 OKRs' task is due 'Today'.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Test Case 1: Faithful Answer\n",
    "answer_faithful = \"The 'Develop 3-year plan' task is due 'This Year.'\"\n",
    "\n",
    "# Test Case 2: Hallucinated Answer\n",
    "answer_hallucinated = \"The 'Develop 3-year plan' task is due 'This Year,' and it must be approved by the CFO before submission.\"\n",
    "\n",
    "# Create datasets\n",
    "dataset_faithful = Dataset.from_dict({\n",
    "    \"question\": [question],\n",
    "    \"answer\": [answer_faithful],\n",
    "    \"contexts\": contexts\n",
    "})\n",
    "\n",
    "dataset_hallucinated = Dataset.from_dict({\n",
    "    \"question\": [question],\n",
    "    \"answer\": [answer_hallucinated],\n",
    "    \"contexts\": contexts\n",
    "})\n",
    "\n",
    "# Evaluate\n",
    "result_faithful = evaluate(dataset_faithful, metrics=[faithfulness], llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "result_hallucinated = evaluate(dataset_hallucinated, metrics=[faithfulness], llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "\n",
    "# Prepare data for comparison\n",
    "scores = {\n",
    "    \"Faithful Answer\\n(Claims supported by context)\": result_faithful[\"faithfulness\"],\n",
    "    \"Hallucinated Answer\\n(Claim about 'CFO approval' unsupported)\": result_hallucinated[\"faithfulness\"]\n",
    "}\n",
    "\n",
    "# Generate comparison chart\n",
    "plot_metric_comparison(\n",
    "    scores_dict=scores,\n",
    "    title='Faithfulness Score: Accurate vs. Hallucinated Answer',\n",
    "    ylabel='Faithfulness Score'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a62210-07d1-4fc7-84e9-5dd7505c151f",
   "metadata": {},
   "source": [
    "**图表解读：**  \n",
    "带有幻觉的回答获得了更低的忠实度得分。关于“需要 CFO 批准”的陈述没有任何上下文支撑，因而拉低了整体得分。\n",
    "\n",
    "### 提升忠实度\n",
    "\n",
    "* **强化提示词：** 明确写出“请仅根据下方上下文作答；若上下文未包含答案，请回答‘I don't know.’”等指令。\n",
    "* **更换 LLM：** 不同 LLM 的幻觉倾向差异很大，可考虑选择更注重扎根事实的模型。\n",
    "* **对答案做后处理：** 加一道验证流程，由另一模型检查回答是否得到上下文支持。\n",
    "\n",
    "## 答案正确度（Answer Correctness）\n",
    "\n",
    "**衡量内容：**   \n",
    "答案正确度是面向用户的“终极指标”。  \n",
    "它回答的问题是：*最终答案与参考答案相比，是否正确且完整？*\n",
    "\n",
    "**重要性：**\n",
    "这是用户真正看到的部分。高正确度意味着系统正在输出有价值的结果。\n",
    "\n",
    "Ragas 将语义相似度（回答是否表达相同含义）与事实准确性（具体事实是否正确）融合为一个分数，使用类似 F1 的方式计算事实重叠度。事实准确性会度量生成答案与参考答案之间事实的重合情况，对应以下概念：\n",
    "\n",
    "答案正确度的计算公式如下：\n",
    "\n",
    "$$\n",
    "    \\text{F1 Score} = {|\\text{TP}| \\over {(|\\text{TP}| + 0.5 \\times (|\\text{FP}| + |\\text{FN}|))}}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* **TP（真正）：** 同时出现在参考答案和生成答案中的事实或陈述。\n",
    "* **FP（假正）：** 仅出现在生成答案而不在参考答案中的事实或陈述。\n",
    "* **FN（假负）：** 仅出现在参考答案而不在生成答案中的事实或陈述。\n",
    "\n",
    "\n",
    "### 直观示例\n",
    "\n",
    "我们来比较一个完美答案、一个部分正确的答案，以及一个错误答案。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97282658-df6b-42fe-bc9a-cbaa83a688d8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "ground_truth = \"4 tasks are due today: Finalize Q3 OKRs, Onboard new team member, Update project roadmap, Write thank-you letter to penpal.\"\n",
    "\n",
    "# Define test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"answer\": \"There are 4 tasks due today: Finalize Q3 OKRs, Onboard new team member, Update project roadmap, and Write thank-you letter to penpal.\",\n",
    "        \"label\": \"Perfect Answer\\n(All facts correct)\"\n",
    "    },\n",
    "    {\n",
    "        \"answer\": \"One task is due today: Finalize Q3 OKRs.\",\n",
    "        \"label\": \"Partially Correct\\n(1 fact correct, 3 missing)\"\n",
    "    },\n",
    "    {\n",
    "        \"answer\": \"No tasks are due today.\",\n",
    "        \"label\": \"Wrong Answer\\n(0 facts correct)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate each case\n",
    "scores = {}\n",
    "for case in test_cases:\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"question\": [question],\n",
    "        \"answer\": [case[\"answer\"]],\n",
    "        \"ground_truth\": [ground_truth]\n",
    "    })\n",
    "    result = evaluate(dataset, metrics=[answer_correctness], llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "    scores[case[\"label\"]] = result[\"answer_correctness\"]\n",
    "\n",
    "# Generate the chart using our reusable function\n",
    "plot_metric_comparison(\n",
    "    scores_dict=scores,\n",
    "    title='Answer Correctness Score for Different Answer Qualities',\n",
    "    ylabel='Answer Correctness Score'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c1f1b-57fe-4828-9f0b-14178ca3aa12",
   "metadata": {},
   "source": [
    "**图表解读：**  \n",
    "得分直接反映最终输出的质量——完整性越高、错误越少，得分越高。\n",
    "\n",
    "### 提升答案正确度\n",
    "\n",
    "* **先解决上游问题：** 答案正确度往往只是症状。利用其他指标（例如低召回）来定位根因。\n",
    "* **优化提示词：** 使用鼓励结构化、完整回答的提示，例如“列出今天到期的所有任务”。\n",
    "* **启用多步推理：** 换用更强大的 LLM，或提示它“逐步思考”，避免遗漏细节。\n",
    "\n",
    "\n",
    "## 从指标中提炼可执行洞见\n",
    "\n",
    "评估的真正价值在于组合这些指标。如果单独看某一个，很容易产生误导。诊断的魔力在于综合分析。\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    classDef frameworkStyle fill:#ffffff,stroke:#1f77b4,stroke-width:2px;\n",
    "\n",
    "    subgraph flowchat [Diagnosis Flowchart]\n",
    "        A[Low Answer Correctness] --> C{Low Context Recall}\n",
    "        C -->|Yes| D[Retrieval Problem: <br>Critical info missing.<br>Fix chunking/embeddings.]\n",
    "        C -->|No| E{Low Context Precision}\n",
    "        E -->|Yes| F[Distracted LLM: <br>Too much noise in context.<br>Improve retrieval filtering/re-ranking.]\n",
    "        E -->|No| G{Low Faithfulness}\n",
    "        G -->|Yes| H[Generation Problem: <br>LLM is hallucinating.<br>Fix prompt or LLM.]\n",
    "        G -->|No| I[Answer Formatting/Completeness Issue: <br>Answer is correct but incomplete or poorly structured.]\n",
    "    end\n",
    "\n",
    "    class flowchat frameworkStyle;\n",
    "```\n",
    "\n",
    "总之，我们可以把已掌握的内容浓缩成一张简单的表格。\n",
    "\n",
    "<table class='diagnostic-table'>\n",
    "    <tr>\n",
    "        <th>指标</th>\n",
    "        <th>高值 (≥0.8)</th>\n",
    "        <th>中值 (0.5–0.8)</th>\n",
    "        <th>低值 (<0.5)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Context Recall</strong></td>\n",
    "        <td class='high'>检索器找到了大部分相关信息</td>\n",
    "        <td class='medium'>错过了一些关键信息</td>\n",
    "        <td class='low'>缺少关键信息——需要修复检索</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Context Precision</strong></td>\n",
    "        <td class='high'>检索到的文本块高度相关</td>\n",
    "        <td class='medium'>检索上下文略有噪声</td>\n",
    "        <td class='low'>无关信息过多——LLM 容易被干扰</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Faithfulness</strong></td>\n",
    "        <td class='high'>回答紧贴检索上下文</td>\n",
    "        <td class='medium'>存在轻微幻觉或遗漏</td>\n",
    "        <td class='low'>严重幻觉——需要修提示或更换模型</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Answer Correctness</strong></td>\n",
    "        <td class='high'>答案事实准确且完整</td>\n",
    "        <td class='medium'>部分正确或不够完整</td>\n",
    "        <td class='low'>答案错误——应检查整条管线</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"4\" class='insight'>\n",
    "            <strong>核心洞见：</strong> \n",
    "            如果 <strong>Context Recall</strong> 低但 <strong>Faithfulness</strong> 高，说明 LLM 没撒谎——它只是拿到错误信息。 \n",
    "            这时应该修检索（块划分、嵌入），而不是怪生成。\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb6ee7-f003-4573-bba2-d1933d0540fb",
   "metadata": {},
   "source": [
    "# 其他评估框架\n",
    "\n",
    "虽然 Ragas 在自动化、LLM 驱动的评估方面表现优异，但它并非唯一的选择。下面我们来看看其他几种常用框架——它们各有千秋。\n",
    "\n",
    "## Ragas（我们的首选）\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    classDef frameworkStyle fill:#ffffff,stroke:#1f77b4,stroke-width:2px;\n",
    "\n",
    "    subgraph Ragas_Framework [Ragas Framework]\n",
    "        direction TB\n",
    "        \n",
    "        subgraph Ragas_Evaluator [Ragas Evaluator]\n",
    "            direction LR\n",
    "            Judge[LLM Judge] -->|Scores| CR[Context Recall]\n",
    "            Judge -->|Scores| CP[Context Precision]\n",
    "            Judge -->|Scores| F[Faithfulness]\n",
    "            Judge -->|Scores| AC[Answer Correctness]\n",
    "        end\n",
    "\n",
    "        Q[User Question] --> Ragas\n",
    "        A[Generated Answer] --> Ragas\n",
    "        C[Retrieved Context] --> Ragas\n",
    "        GT[Ground Truth] --> Ragas\n",
    "        Ragas --> Judge\n",
    "        \n",
    "    end\n",
    "    \n",
    "    class Ragas_Framework frameworkStyle;\n",
    "    \n",
    "    style Judge fill:#e1f5fe,stroke:#1f77b4\n",
    "    style Ragas stroke:#1f77b4,stroke-width:2px\n",
    "```\n",
    "\n",
    "> Ragas 使用独立的 LLM 作为“裁判”，自动为 RAG 系统的输出打分。它需要参考答案进行对比，可快速给出整体性能评估。\n",
    "\n",
    "Ragas 是专为 RAG 应用打造的开源框架，其优势在于一组针对 RAG 的指标，例如上下文召回、上下文精度、忠实度与答案相关性，可直击检索增强系统的常见薄弱点。借助“LLM 裁判”，Ragas 能为生成答案的质量及其与检索上下文的扎根程度提供细腻的评分，因此非常适合快速获得 RAG 管线整体健康状况的自动化评估。不过，由于它依赖 LLM 评估器，得分有时会像“黑箱”，让人难以弄清具体扣分原因，从而增加调试难度。\n",
    "\n",
    "## TruLens\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    classDef frameworkStyle fill:#ffffff,stroke:#1f77b4,stroke-width:2px;\n",
    "    \n",
    "    subgraph TruLens_Framework [TruLens Framework]\n",
    "        A[User Query] --> B[TruLens Monitor]\n",
    "        B --> C[Chunking & Retrieval]\n",
    "        B --> D[Prompt Assembly]\n",
    "        B --> E[LLM Generation]\n",
    "        B --> F[Final Answer]\n",
    "\n",
    "        subgraph TruLens Dashboard\n",
    "            G[Log: Query]\n",
    "            H[Log: Retrieved Chunks]\n",
    "            I[Log: Final Prompt]\n",
    "            J[Log: LLM Call]\n",
    "            K[Log: Answer]\n",
    "            G --> H --> I --> J --> K\n",
    "        end\n",
    "\n",
    "        B --> G\n",
    "        B --> H\n",
    "        B --> I\n",
    "        B --> J\n",
    "        B --> K\n",
    "    \n",
    "    end\n",
    "        \n",
    "    class TruLens_Framework frameworkStyle;\n",
    "```\n",
    "\n",
    "> TruLens 像飞行记录仪一样捕捉 RAG 管线的每个步骤，非常适合深入调试并定位问题所在。\n",
    "\n",
    "TruLens 的特色在于可观测性与可追踪性。它不仅提供最终得分，更像 LLM 应用的“黑匣子”，从最初的查询到最终答案、包括中间的提示词、检索上下文与模型调用，都会被记录下来。这种细节对调试复杂问题极为宝贵。凭借与 LlamaIndex 等框架的集成，开发者可以设置反馈函数，在不同阶段程序化地评估扎根性与相关性。虽然 TruLens 内置指标不如其他框架丰富，也需要更多配置，但它为整个管线提供的全景视图对定位问题极其有用。\n",
    "\n",
    "## DeepEval\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph DeepEval Framework\n",
    "        T[Define Test Cases] -->|Question, Ground Truth| DE[DeepEval]\n",
    "        DE -->|Run Tests| A1[Test: Answer Correctness]\n",
    "        DE -->|Run Tests| A2[Test: Faithfulness]\n",
    "        DE -->|Run Tests| A3[Test: Context Recall]\n",
    "\n",
    "        A1 -->|Pass/Fail| R[CI/CD Pipeline]\n",
    "        A2 -->|Pass/Fail| R\n",
    "        A3 -->|Pass/Fail| R\n",
    "    end\n",
    "    \n",
    "    style DE fill:#ffecb3,stroke:#f57c00,stroke-width:2px\n",
    "    style R fill:#e8f5e8,stroke:#2e7d32\n",
    "```\n",
    "> DeepEval 将 RAG 评估视作软件测试。你定义断言，它执行自动化测试，非常适合在生产环境中防止回归。 \n",
    "\n",
    "DeepEval 采用软件工程思路，被称为“面向 LLM 的 Pytest”。它倡导测试驱动开发（TDD），鼓励你在构建 RAG 应用之前或同时编写评估测试。该框架在 CI/CD 场景中表现突出，可以设定严格的通过/失败标准（例如“faithfulness > 0.9”）来防止退化。DeepEval 支持多种 LLM 任务的指标，不限于 RAG，因此对于构建多类型 LLM 功能的团队尤为实用。其结构化、断言式的测试模型为生产级质量把控提供了高度的自动化与掌控力。\n",
    "\n",
    "## 自定义评估框架\n",
    "\n",
    "当标准指标无法覆盖你应用的特定需求时，自定义评估框架就显得不可或缺。例如，检查回答中是否包含指定链接、是否遵循某个工作流程，甚至是否符合预期情绪语调。自建方案虽然投入最大，却能让评估与业务目标和用户体验精确对齐。值得注意的是，这些框架并非互斥；成熟的评估策略通常组合使用，例如用 Ragas 做广覆盖的自动打分、用 TruLens 深挖异常案例，再用自定义规则保障关键业务逻辑。\n",
    "\n",
    "\n",
    "## 框架对比\n",
    "\n",
    "| 功能 | **Ragas** | **TruLens** | **DeepEval** | **自定义框架** |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **最适用场景** | 为 RAG 系统提供端到端的自动化指标评分。 | 提供可观测性、调试能力，追踪整条 RAG 管线。 | 测试驱动开发（TDD）与 CI/CD 集成。 | 强制执行特定业务逻辑与领域规则。 |\n",
    "| **优势** | 轻量、易集成，指标专为 RAG 设计（如上下文召回、忠实度），利用 LLM 裁判输出细腻评分。 | 拥有可视化面板，记录管线每一步，与 LangChain、LlamaIndex 集成良好，可实时反馈。 | 设计理念类似软件测试框架（如 pytest），支持断言与自动回归测试。 | 最大的灵活性，可定义现成工具无法覆盖的独特指标（如政策合规、情绪语调）。 |\n",
    "| **劣势** | 需要能力足够的评估型 LLM；评分可能“黑箱”，低分难以解释；指标含义不一定直观。 | 内置指标相对较少；相比简单工具需要更多配置。 | 对复杂自定义逻辑可能需要更多设置；侧重验证而非深度管线分析。 | 开发与维护成本高，需要大量工程投入。 |\n",
    "| **适用用例** | 快速获取 RAG 管线性能的自动化得分，定位宏观问题。 | 通过检查中间步骤、提示词和检索上下文，深入调试故障管线。 | 在生产环境中自动化质量检查，确保更新不会破坏既有功能。 | 验证 AI 回答是否遵循严格业务规则，例如包含指定链接或遵循客服话术。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886894e0-b23f-4cb7-ab41-adbee01ab099",
   "metadata": {},
   "source": [
    "# 建立评估框架的最佳实践\n",
    "\n",
    "---\n",
    "\n",
    "你已经掌握了一套强大的工具：手动检查用来诊断问题、Ragas 的自动化指标帮助你在规模上衡量表现、还有可以选择的多种框架。但真正的挑战不是“会用这些工具”，而是打造一套可持续、有效的评估实践，推动持续改进。\n",
    "\n",
    "把它想象成维护一台高性能引擎。再先进的诊断工具，如果没有定期保养、专业技师和详尽记录，发动机最终也会出故障。\n",
    "\n",
    "以下是为 RAG 应用构建稳健评估体系的关键步骤。\n",
    "\n",
    "## 让领域专家尽早参与\n",
    "\n",
    "成功的评估策略，最重要的因素是人的专业判断。再强大的 LLM 判断器，也比不过熟悉业务细节的专家。\n",
    "\n",
    "> **重要性：**  \n",
    "> 只有真正深入业务的人才能界定什么才算“正确”。“高优先级”的任务是否比“今日截止”更紧急？回答是否必须包含链接、截止日期或审批流程？\n",
    "\n",
    "### 实践方式：\n",
    "\n",
    "* **共同编写 ground truth：** 不要单独写 `ground_truth`。与专家（如 HR 经理、项目负责人）坐下来一起撰写理想回答。\n",
    "* **识别边缘情景：** 专家最了解棘手场景。例如“任务已过期但状态仍是 Not Started？”或“刚删除的任务被询问怎么办？”\n",
    "* **审查指标：** 定期请专家复核低分案例，确认是指标判断正确，还是应该更新参考答案。\n",
    "\n",
    "> **💡 专家提示：**  \n",
    "> 把领域专家当成副驾驶，而不是事后审阅者。他们的洞见应该从第一天就融入评估标准。\n",
    "\n",
    "## 用真实用户查询构建测试集\n",
    "\n",
    "评估数据集是质量保障的基石。如果它源于假设问题，评估结果也难以反映真实表现。\n",
    "\n",
    "**真实查询来自哪里：**\n",
    "\n",
    "* **聊天记录：** 挖掘客服机器人、内部聊天工具或早期用户测试的对话。\n",
    "* **搜索历史：** 查看用户在知识库或 Wiki 中搜索的内容。\n",
    "* **工单：** 支持工单是用户痛点和模糊表达的宝库。\n",
    "\n",
    "### 聚焦高价值问题：\n",
    "\n",
    "**优先关注：**\n",
    "\n",
    "* **高频问题：** 经常出现的提问。\n",
    "* **高风险问题：** 与截止时间、政策或财务信息相关的请求。\n",
    "* **模糊问题：** 多种解释的提问（例如“本周截止的是什么？” vs.“今天截止的是什么？”）。\n",
    "\n",
    "这样才能让评估努力聚焦在对用户最重要的领域。\n",
    "\n",
    "## 混合使用多种评估方法\n",
    "\n",
    "没有任何单一工具能呈现全貌。最有效的团队会采用分层诊断组合：\n",
    "\n",
    "| 方法 | 目的 | 优势 | 局限性 |\n",
    "|--------|--------|-----------|-------------|\n",
    "| **Manual Inspection** | 直观验证检索与输出质量 | 快速、直观，能发现格式问题 | 难以扩展、主观性强 |\n",
    "| **TruLens** | 追踪并调试完整 RAG 管线 | 展示提示词、上下文、LLM 调用 | 不直接给出质量得分 |\n",
    "| **Ragas** | 用指标量化性能 | 提供客观评分（正确度、忠实度） | 需要参考答案 |\n",
    "| **LLM-as-a-Judge** | 自动评估回答质量 | 可衡量语气、相关性、完整性 | 可能出现评估幻觉 |\n",
    "| **A/B Testing** | 比较提示或检索策略变化 | 能衡量真实用户影响 | 需要真实流量与指标 |\n",
    "\n",
    "> 💡 **最佳实践**：结合 **TruLens 调试** + **Ragas 打分** + **人工审查边缘案例**。\n",
    "### 示例工作流：\n",
    "\n",
    "1. 使用 Ragas 做日常自动化运行，及时发现性能下降。\n",
    "2. 当得分偏低时，借助 TruLens 深入日志，定位具体问题。\n",
    "3. 编写 DeepEval 测试，确保该问题不会再次出现。\n",
    "4. 面向新功能（例如退款政策），编写自定义评估器，确保回答始终包含指定链接。\n",
    "\n",
    "## 闭合反馈回路\n",
    "\n",
    "评估不是一次性任务，而是持续改进的循环。目标是构建一个飞轮，每转一圈，系统就更可靠。\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Quality_Flywheel [Quality Flywheel]\n",
    "        A[Collect Bad Cases] --> B[Analyze Metrics]\n",
    "        B --> C[Improve Chunking/Prompting]\n",
    "        C --> D[Re-evaluate]\n",
    "        D --> E[Deploy]\n",
    "        E --> A\n",
    "    end\n",
    "```\n",
    "\n",
    "### 实施质量飞轮\n",
    "\n",
    "* **Collect：** 汇集失败用例、用户反馈与低分查询。\n",
    "* **Analyze：** 利用指标诊断根因（例如召回低 → 修检索）。\n",
    "* **Improve：** 有针对性地修改（调整块大小、重写提示等）。\n",
    "* **Re-evaluate：** 运行测试集验证修复效果。\n",
    "* **Deploy：** 发布改进版本。\n",
    "* **Repeat：** 不断循环，持续增强系统可靠性。\n",
    "\n",
    "## 把评估当成产品\n",
    "\n",
    "评估框架与应用代码一样重要，需要同样严格对待。\n",
    "\n",
    "* **版本控制：** 将测试用例、参考答案与评估脚本与主代码一起存入 Git。\n",
    "* **完整文档：** 说明每个测试用例为何存在、旨在捕捉什么问题。\n",
    "* **监控趋势：** 持续追踪关键指标（如平均答案正确度）。分数突然下降就是预警。\n",
    "* **接入 CI/CD：** 将评估套件集成到部署流程中。例如，当忠实度低于 0.8 时直接阻止构建。\n",
    "\n",
    "# 接下来做什么？\n",
    "\n",
    "---\n",
    "\n",
    "## 答题自测！\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
    "<b>1. 你的 RAG 系统给出了正确答案，但 `context_recall` 得分很低。最可能的原因是什么？</b>  \n",
    "\n",
    "<ul>\n",
    "    <li>A) LLM 被无关文本块干扰  </li>\n",
    "    <li>B) 回答不完整  </li>\n",
    "    <li>C) 检索到的上下文不包含必要信息  </li>\n",
    "    <li>D) LLM 在幻觉</li>\n",
    "</ul>\n",
    "\n",
    "**查看答案 →**\n",
    "</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
    "\n",
    "✅ **正确答案：** C) 检索到的上下文不包含必要信息  \n",
    "📝 **说明**：\n",
    "* `context_recall` 低意味着正确信息没有被检索到。如果答案依然正确，那就只能是模型“凭空补充”了事实——这是一种危险的失败模式，说明检索已出问题，而 LLM 在帮它“擦屁股”。\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "## 要点回顾\n",
    "\n",
    "* **为什么要评估 RAG 系统？**\n",
    "    * **RAG 是一条管线，而非单一步骤**——涉及分块、检索、上下文组装与生成。\n",
    "    * **当事情出错时，根因往往不明**——是模型幻觉，还是根本没检索到正确上下文？\n",
    "    * **评估能区分症状与根因：**\n",
    "      - 错误答案可能来自糟糕的检索、提示或模型限制。\n",
    "    * **没有评估就像蒙着眼上线**——看不到问题也就无法改进。\n",
    "    * **评估带来信任、可扩展性与迭代能力**——这是生产级 AI 的基础。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **手动评估**\n",
    "    * **视觉检查是第一步**\n",
    "    * **留意预警信号：**\n",
    "      - 缺少关键关键词\n",
    "      - 文本块覆盖不全\n",
    "      - 文本被破坏或乱码\n",
    "    * **手动调试能培养直觉**——理解检索与生成如何互动。\n",
    "    * **始终检查原始节点**——确认模型是否拿到了正确信息，再决定是否怪 LLM。\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "* **使用 Ragas 做自动化评估**\n",
    "    * **Ragas 提供量化指标**——把主观质量转化为可测分数。\n",
    "    * 核心指标：\n",
    "      - **Answer Correctness**：回答在事实层面有多准确。\n",
    "      - **Context Recall**：检索上下文是否包含所有相关事实。\n",
    "      - **Context Precision**：检索文本块是否相关，是否存在大量噪声。\n",
    "      - **Faithfulness**：回答是否源自上下文，而非幻觉。\n",
    "    * **Ragas 让评估进入 CI/CD 流程**——测试、打分、放心上线。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **构建贴近真实的评估数据集**\n",
    "    * **使用真实用户提问**——不要凭空杜撰：\n",
    "      - 聊天记录\n",
    "      - 搜索历史\n",
    "      - 支持工单\n",
    "    * **加入模糊、含糊或多问一题的场景**——这些最难也最能暴露问题。\n",
    "    * **定义清晰的参考答案**——每个问题的“正确答案”是什么？\n",
    "    * **把领域专家当副驾驶**——他们的见解应从第一天起塑造你的评估标准。\n",
    "    * **小规模起步，但要尽早**——哪怕只有 10 个高质量用例，也能揭示系统性问题。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **解读指标**\n",
    "    * **指标是诊断工具，不是成绩单：**\n",
    "      - `context_recall` 低 → 修检索（优化分块、嵌入或搜索）\n",
    "      - `answer_correctness` 低 → 优化提示或换更强的模型\n",
    "      - `context_precision` 高但 `answer_correctness` 低 → 模型在忽略优质上下文\n",
    "    * **不要孤立看指标**——组合分析才能找到根因。\n",
    "    * **举例：** 答案正确 + 召回低 = 幻觉（危险！）\n",
    "    * **评估是迭代的**——测量、修复、再测量。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **从评估中提炼可执行洞见**\n",
    "    * **先修上游问题**——答案正确度往往只是症状。结合其他指标定位根因：\n",
    "      - 检索失败？→ 改进分块或嵌入\n",
    "      - 提示欠佳？→ 加入结构、示例或链式思考\n",
    "    * **优化提示**——采用如下指令：\n",
    "      - “列出今天到期的所有任务。”\n",
    "      - “请用项目符号作答。”\n",
    "    * **启用多步推理**——使用具备思考模式的推理模型（如 Qwen-plus），减少遗漏细节。\n",
    "    * **评估的真正价值**不在分数，而在于这些分数激发的**洞见**，它们能促成更合理的系统设计。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4571b6-5dfb-49af-bc7e-2e30462f849e",
   "metadata": {},
   "source": [
    "# 下一步？\n",
    "\n",
    "---\n",
    "\n",
    "## 自测小练！\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
    "<b>1. 你的 RAG 系统给出了正确答案，但 `context_recall` 得分很低。最可能的原因是什么？</b>  \n",
    "\n",
    "<ul>\n",
    "    <li>A) LLM 被无关文本块干扰  </li>\n",
    "    <li>B) 回答不完整  </li>\n",
    "    <li>C) 检索到的上下文不包含必要信息  </li>\n",
    "    <li>D) LLM 在幻觉</li>\n",
    "</ul>\n",
    "\n",
    "**查看答案 →**\n",
    "</summary>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
    "\n",
    "✅ **正确答案：** C) 检索到的上下文不包含必要信息  \n",
    "📝 **说明**：\n",
    "* `context_recall` 低意味着正确信息没有被检索到。如果答案依然正确，那只能说明模型“凭空补充”了事实——这是一种危险的失败模式，表示检索阶段已经出问题，而 LLM 在替它“擦屁股”。\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "## 要点回顾\n",
    "\n",
    "* **为什么要评估 RAG 系统？**\n",
    "    * **RAG 是一条管线，而非单一步骤**——涉及分块、检索、上下文组装与生成。\n",
    "    * **当事情出错时，根因往往不明**——是模型幻觉，还是压根没检索到正确上下文？\n",
    "    * **评估能区分症状与根因：**\n",
    "      - 错误答案可能来自糟糕的检索、提示或模型限制。\n",
    "    * **没有评估就像蒙着眼上线**——看不到问题也就无法改进。\n",
    "    * **评估带来信任、可扩展性与迭代能力**——这是生产级 AI 的基石。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **手动评估**\n",
    "    * **视觉检查是第一步**\n",
    "    * **留意预警信号：**\n",
    "      - 缺少关键关键词\n",
    "      - 文本块覆盖不全\n",
    "      - 文本被破坏或乱码\n",
    "    * **手动调试能培养直觉**——理解检索与生成如何互动。\n",
    "    * **始终检查原始节点**——确认模型是否拿到了正确信息，再决定是否怪 LLM。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **使用 Ragas 做自动化评估**\n",
    "    * **Ragas 提供量化指标**——把主观质量转化为可测分数。\n",
    "    * 核心指标：\n",
    "      - **答案正确度（Answer Correctness）**：回答在事实层面有多准确。\n",
    "      - **上下文召回率（Context Recall）**：检索上下文是否包含所有相关事实。\n",
    "      - **上下文精度（Context Precision）**：检索文本块是否相关，是否存在大量噪声。\n",
    "      - **忠实度（Faithfulness）**：回答是否源自上下文，而非幻觉。\n",
    "    * **Ragas 让评估进入 CI/CD 流程**——测试、打分、放心上线。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **构建贴近真实的评估数据集**\n",
    "    * **使用真实用户提问**——不要凭空杜撰：\n",
    "      - 聊天记录\n",
    "      - 搜索历史\n",
    "      - 支持工单\n",
    "    * **加入模糊、含糊或多问一题的场景**——这些最难也最能暴露问题。\n",
    "    * **定义清晰的参考答案**——每个问题的“正确答案”是什么？\n",
    "    * **把领域专家当副驾驶**——他们的见解应从第一天起塑造你的评估标准。\n",
    "    * **小规模起步，但要尽早**——哪怕只有 10 个高质量用例，也能揭示系统性问题。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **解读指标**\n",
    "    * **指标是诊断工具，不是成绩单：**\n",
    "      - `context_recall` 低 → 修检索（优化分块、嵌入或搜索）\n",
    "      - `answer_correctness` 低 → 优化提示或换更强的模型\n",
    "      - `context_precision` 高但 `answer_correctness` 低 → 模型在忽略优质上下文\n",
    "    * **不要孤立看指标**——组合分析才能找到根因。\n",
    "    * **举例：** 答案正确 + 召回低 = 幻觉（危险！）\n",
    "    * **评估是迭代的**——测量、修复、再测量。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **从评估中提炼可执行洞见**\n",
    "    * **先修上游问题**——答案正确度往往只是症状。结合其他指标定位根因：\n",
    "      - 检索失败？→ 改进分块或嵌入\n",
    "      - 提示欠佳？→ 加入结构、示例或链式思考\n",
    "    * **优化提示**——采用如下指令：\n",
    "      - “列出今天到期的所有任务。”\n",
    "      - “请用项目符号作答。”\n",
    "    * **启用多步推理**——使用具备思考模式的推理模型（如 Qwen-plus），减少遗漏细节。\n",
    "    * **评估的真正价值**不在分数，而在于这些分数激发的**洞见**，它们能促成更合理的系统设计。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskfriend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
