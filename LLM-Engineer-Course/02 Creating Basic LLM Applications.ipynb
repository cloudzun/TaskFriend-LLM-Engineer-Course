{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff20e6c-a14d-4c53-9efd-02ac5f1142ac",
   "metadata": {},
   "source": [
    "# 构建基础 LLM 应用\n",
    "\n",
    "---\n",
    "\n",
    "在上一章我们讨论了大型语言模型（LLM）的体系结构，接下来让我们动手构建一个可落地的 LLM 应用。现代 LLM 与传统 AI 系统在多个维度存在本质区别：\n",
    "\n",
    "* **泛化能力：** LLM 能够利用预训练阶段积累的知识，以极少甚至无需额外训练的成本适配新任务。\n",
    "* **流式输出：** LLM 支持按词元实时生成回答，逐字呈现，营造出自然的交互体验，而非传统的批处理模式。\n",
    "* **上下文窗口：** LLM 会在有限的上下文长度内处理输入（远超传统 AI），这决定了它在生成回答时可以参考多少既往对话或文档内容。\n",
    "\n",
    "本章我们将围绕上一章登场的 AI 助手 **TaskFriend** 构建一款应用。重点涵盖多轮对话、系统提示（system prompt）设计等核心功能，以塑造稳定且可控的 AI 行为。\n",
    "\n",
    "## 故事进展\n",
    "\n",
    "您已掌握 LLM 架构的基础，准备为应用打造新特性。应用本身具备任务管理功能，但您希望进一步帮助用户分析任务、区分优先级并拆解大型目标，于是决定加入一个聊天机器人来辅助完成这些工作。\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "* 体验流式响应带来的自然对话体验\n",
    "* 实现多轮会话，使交互能够跨轮次保持上下文\n",
    "* 理解系统提示如何定义并约束 AI 行为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7eff0-18ae-485c-8847-5cc9e7ea77c4",
   "metadata": {},
   "source": [
    "## 初始化环境\n",
    "\n",
    "### 配置 API Key\n",
    "\n",
    "在任何笔记本开始工作前，都需要加载 [Model Studio 的 API Key](https://modelstudio.console.alibabacloud.com/?tab=globalset#/efm/api_key)，以便调用本课程将持续使用的 Qwen 模型接口。\n",
    "\n",
    "> 如不清楚如何获取 **Model Studio** API Key，请参考 `00 Setting Up the Environment` 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6fadb-2387-494b-bf2b-60bc36e86ac6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Model Studio API key\n",
    "import os\n",
    "from config.load_key import load_key\n",
    "load_key(\n",
    "    confirmation=False       # ← Change to \"True\" if you want to change your API key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6aaef2-dcfc-4e22-b76d-4cf81726b698",
   "metadata": {},
   "source": [
    "### 配置 LLM 客户端\n",
    "\n",
    "我们将使用 DashScope 提供的、兼容 OpenAI 协议的接口，与 Qwen 模型（以及课程中后续涉及的其他模型）进行交互。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e96a65-3e1c-4cfe-9255-919794986eb0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af34c61-9e24-4f83-a769-6e7662b00029",
   "metadata": {},
   "source": [
    "### 配置流式响应函数\n",
    "\n",
    "我们在上一章已经实现了 `get_qwen_stream_response`，此处可以直接复用该方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd83428-67f0-45e9-98fd-68da6a7a478a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Streaming mode response\n",
    "def get_qwen_stream_response(query, system_prompt, temperature, top_p):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen-plus\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    for chunk in response:\n",
    "        try:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:  # Skip empty chunks\n",
    "                yield content  \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing chunk: {e}\")\n",
    "            yield f\"Error parsing response: {e}\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbcf327-8538-45d1-a826-3b1d996503af",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User query\n",
    "query = \"Tell me about yourself\"\n",
    "\n",
    "# System prompt\n",
    "system_prompt = \"You are TaskFriend, a helpful AI assistant that helps users manage daily tasks, prioritize work, and optimize time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaaf5fe-853f-43fa-b60f-e7ac0a00882a",
   "metadata": {},
   "source": [
    "# 多轮对话：它如何工作？\n",
    "\n",
    "---\n",
    "\n",
    "传统 AI 系统通常只能处理一次性的问题，例如 “今天的天气如何？”。但现实中的效率管理往往需要多次澄清、调整与反馈，例如规划繁忙的一周或拆解大型项目。多轮对话允许 LLM 像人类助理一样维持上下文，完成这些复杂而动态的交互。\n",
    "\n",
    "对于 **TaskFriend** 而言，这意味着它能够把用户模糊的诉求（如“事情太多，不知从何入手”）逐步引导为明确、可执行的行动计划。\n",
    "\n",
    "要构建可靠的多轮对话应用，既要理解底层原理，也要掌握缓解局限的实用策略。\n",
    "\n",
    "## 上下文窗口\n",
    "\n",
    "相比传统 AI 每轮重置的做法，现代 LLM 能跨多轮理解并记住对话内容。秘诀就在于 **上下文窗口（context window）**。\n",
    "\n",
    "上下文窗口定义了模型在单次请求中能够“记住”的既往对话与信息量。例如，用户先说 “我下周要准备一个演示”，随后又问 “还能挤出时间去健身吗？”——模型要判断是否腾得出时间，就必须记住先前提到的演示。\n",
    "\n",
    "Qwen 系列模型支持超大的上下文窗口（如 [Qwen3](https://qwenlm.github.io/blog/qwen3/) 最高可达 128K 词元），因此可以保留较长的对话历史、任务清单、日程约束与健康偏好。不过，随着对话增长，开发者必须监控词元使用情况，避免关键内容被截断。\n",
    "\n",
    "下图以 **TaskFriend** 应用为例，展示了上下文如何在多轮对话中逐渐累积：\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    S[\"System Prompt: 'You are TaskFriend…'\"] --> C[\"Context Window\"]\n",
    "    U1[\"User: 'I need to finish a report due Friday'\"] --> C\n",
    "    A1[\"Assistant: 'How many hours do you have free this week?'\"] --> C\n",
    "    U2[\"User: 'About 10 hours, but I want to exercise daily'\"] --> C\n",
    "    A2[\"Assistant: 'Let’s block 2-hour focus sessions and 30-minute workouts'\"] --> C\n",
    "    C --> M[\"Model generates next response\"]\n",
    "\n",
    "```\n",
    "\n",
    "每条消息都会进入上下文，使 **TaskFriend** 能记住历史内容，并在用户回归时从同一话题继续。\n",
    "\n",
    "## 构建多轮会话函数\n",
    "\n",
    "我们已开发出一版简单的 **TaskFriend** 聊天界面，接下来需要强化上一章的 `get_qwen_stream_response` 函数。好消息是：实现比想象中简单。\n",
    "\n",
    "核心在于**维护上下文**——不仅要发送最新消息，还要记录既往对话。这正是从一次性问答转变为真正“助理”的关键。\n",
    "\n",
    "具体步骤如下：\n",
    "\n",
    "1. **接收完整对话历史**，而非仅传递最新一条。\n",
    "2. **以流式方式输出**，让回复实时呈现。\n",
    "3. **累积完整回答**，以便保存回对话历史，供后续调用。\n",
    "\n",
    "下面是增强后的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f4fe4-a88f-45a2-b488-301f105e24f6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Streaming mode response\n",
    "# def get_qwen_stream_response(query, system_prompt, temperature, top_p):\n",
    "\n",
    "# Renaming the function for better visibility\n",
    "def get_qwen_stream_response_accumulate(query, system_prompt, temperature, top_p):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen-plus\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "    for chunk in response:\n",
    "        try:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                full_response += content\n",
    "                yield content\n",
    "        except Exception as e:\n",
    "            yield f\"[ERROR] {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16dc15-8a3b-42ba-a64e-34440beafe86",
   "metadata": {},
   "source": [
    "表面上看，新函数与原函数极为相似——这是刻意设计的结果。唯一新增的是 `full_response`，用于在流式输出的同时累积完整回答。\n",
    "\n",
    "### 为什么需要它？\n",
    "\n",
    "虽然 `yield content` 能即时把内容逐词元地呈现在界面上，营造“正在写作”的动效，但我们仍需将完整回复写回对话历史。如果缺少这一步，**TaskFriend** 无法记住自己说过的话，下一轮就会丢失上下文。\n",
    "\n",
    "> 💡 可以把它比作白板：\n",
    " > * `yield content` 像是拿着记号笔正在书写。\n",
    " > * `full_response` 则是把完整句子记录下来，以备后续查阅。\n",
    "\n",
    "为便于测试与使用，我们为 **TaskFriend** 提供了聊天界面，可通过 `call_llm_fn` 接入 `get_qwen_stream_response_accumulate`（或您自定义的任意函数）。让我们试运行看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd72c8d-a062-45a7-b3ad-a4a6d8913509",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from taskfriend import chat, config\n",
    "from taskfriend.chat import wrap_streaming_for_chat, wrap_rag_for_chat\n",
    "\n",
    "# Wrap function for compatibility\n",
    "wrapped_llm = wrap_streaming_for_chat(\n",
    "    get_qwen_stream_response_accumulate\n",
    ")\n",
    "\n",
    "chat.chat_interface(\n",
    "    full_conversation=[],\n",
    "    client=client,\n",
    "    call_llm_fn=wrapped_llm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc415f8-1e78-4018-871a-a724e5a0122a",
   "metadata": {},
   "source": [
    "现在，每当您输入消息时，**TaskFriend** 会读取完整上下文，流式输出回答，并将结果写回历史记录。您可以继续追问：\n",
    "\n",
    "> “如何在这些任务之间找到平衡？”\n",
    "\n",
    "它会记住先前提到的任务与冲突，因为所有内容都保存在对话历史中。\n",
    "\n",
    "不过要注意：**上下文并非无限**。\n",
    "\n",
    "随着对话增长，我们终会遇到模型的 **上下文窗口** 限制——即一次能处理的最大词元数。若不做管理，较早的信息会被静默丢弃，**TaskFriend 便会遗忘**。\n",
    "\n",
    "### 认识上下文窗口\n",
    "\n",
    "那么幕后到底发生了什么？既然上下文是有限的，当会话超出窗口大小时会怎样？我们可以在 **TaskFriend** 应用中加入新参数，将上下文窗口设为 400 词元，以可视化超限后的行为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b4d9f-ccd1-4cf4-95a5-189962456d67",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat.chat_interface(\n",
    "    full_conversation=[],\n",
    "    client=client,\n",
    "    call_llm_fn=wrapped_llm,\n",
    "    use_context_window=True,\n",
    "    context_window=400,            # Does not take effect when use_context_window=False\n",
    "    show_truncated=True,\n",
    "    show_context_preview=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61150e-b266-45d8-829b-0d2a043bbdac",
   "metadata": {},
   "source": [
    "开始聊天后，您会看到一段新提示：显示实际发送给模型的上下文预览。\n",
    "\n",
    "```\n",
    "===================================\n",
    "ACTUAL CONTEXT SENT TO LLM\n",
    "===================================\n",
    "  [ 0] System   ( 50t) → You are TaskFriend, a helpful AI...\n",
    "  [ 1] User     ( 80t) → I need to finish a report by Friday...\n",
    "  [ 2] Assistant (120t) → That sounds important! How many hours...\n",
    "  [ 3] User     ( 90t) → I also have to prep for a team meeting...\n",
    "  \n",
    "📊 Total: 340/400 tokens used\n",
    "===================================\n",
    "```\n",
    "\n",
    "这正是 LLM 此刻所看到的全部信息。\n",
    "\n",
    "继续输入新消息。随着对话延长，您最终会看到如下提示：\n",
    "\n",
    "```\n",
    "🗑️  TRUNCATED MESSAGES (not sent to model):\n",
    "──────────────────────────────────\n",
    "User: I wanted to start learning Spanish this month...\n",
    "Assistant: That’s a great goal! Maybe 15 minutes a day...\n",
    "──────────────────────────────────\n",
    "```\n",
    "\n",
    "**💥 某些消息被截断了。**\n",
    "\n",
    "即使它们仍旧保存在 `full_conversation` 列表中，也因为距离过远而无法放入 400 词元的窗口。系统必须在“保留最近的对话”与“保留较旧的历史”之间做出选择，而它选择了前者。\n",
    "\n",
    "起初似乎无关紧要，但想象一下：**TaskFriend** 若忘记您已将 7:00–8:00 设为深度工作时段，或忘了您不喜欢周五开会，就会造成干扰甚至反效果。\n",
    "\n",
    "那么该如何应对？\n",
    "\n",
    "* 我们不可能无限扩大上下文窗口（即便 128K 词元终会耗尽）。\n",
    "* 也没办法保留全部消息——我们必须**记住关键信息**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872171ba-f43e-4330-acb6-206d566e2b8c",
   "metadata": {},
   "source": [
    "# 总结：多面手的解决方案\n",
    "\n",
    "---\n",
    "\n",
    "我们已经看到，**当对话过长时会触碰上下文窗口**，导致消息被丢弃。\n",
    "\n",
    "如果我们能记住这些消息的**核心含义**——关键决策、目标与约束——并将它们重新注入对话，会发生什么？这便是 **摘要** 的作用。\n",
    "\n",
    "例如：\n",
    " > “用户计划在周五前完成报告，安排两小时专注时段，并避免周五会议；还想每天学习 15 分钟西班牙语。”\n",
    "\n",
    "这段话大约只占 30 个词元，却保留了上百词元对话中的核心要点。\n",
    "\n",
    "摘要正是把“海量信息”转化为“高价值记忆”的方法。\n",
    "\n",
    "接下来，我们将为 **TaskFriend** 增加摘要函数，打造一套**持久记忆**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61e30c-b3b3-4dde-a702-a8fd0a588c13",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_summarizer(text, client):\n",
    "    print(\"💡 Summarizing dropped conversation...\", flush=True)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen-plus\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "            Summarize the key points from this conversation.\n",
    "            Focus on tasks mentioned, decisions made, goals, and action items.\n",
    "            Be concise — max 4 sentences.\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=100,             # Limit the length of the output, try to aim for 1/4 or less of total context\n",
    "        top_p=0.9,\n",
    "        stream=False\n",
    "    )\n",
    "    summary = response.choices[0].message.content.strip()\n",
    "    print(f\"📝 Summary: {summary}\", flush=True)\n",
    "    return summary\n",
    "\n",
    "# Launch chat with the summarizer we built\n",
    "chat.chat_interface(\n",
    "    full_conversation=[],\n",
    "    client=client,\n",
    "    call_llm_fn=wrapped_llm,\n",
    "    use_context_window=True,\n",
    "    context_window=400,            # Does not take effect when use_context_window=False\n",
    "    show_truncated=True,\n",
    "    show_context_preview=True,\n",
    "    summarize_dropped=True,\n",
    "    summarizer_fn=my_summarizer    # ← Plug in your summarizer!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b3cb9-91c2-4c79-9893-8ea17644ccff",
   "metadata": {
    "tags": []
   },
   "source": [
    "现在，当旧消息即将被丢弃时，TaskFriend 不再直接删除，而是先行生成摘要。\n",
    "\n",
    "您将看到如下输出：\n",
    "\n",
    "```\n",
    "💡 Summarizing dropped conversation...\n",
    "📝 Summary: User needs to finish a report by Friday and prep for a team meeting. Agreed to block 2-hour focus sessions and avoid meetings on Fridays. Also wants to start learning Spanish with 15-min daily practice.\n",
    "```\n",
    "\n",
    "随后，这段摘要会被插入上下文的系统消息之后。\n",
    "\n",
    "尽管原始消息已被移除，TaskFriend 仍然掌握着：\n",
    "\n",
    "* 报告的截止日期是周五\n",
    "* 您不希望在周五安排会议\n",
    "* 您想开始学习西班牙语\n",
    "\n",
    "这就像给 AI 准备了一本**笔记本**——记录的是关键事实，而非逐字对话。\n",
    "\n",
    "> **💡 小贴士：** 可以多次总结，如滚动摘要，来应对更长的对话。\n",
    "\n",
    "这一技巧已在众多 AI 助手中广泛应用——而现在，您已亲手实现。\n",
    "\n",
    "## 摘要的其他优势\n",
    "\n",
    "摘要不仅能节省词元，还能大幅增强 LLM 应用的稳健性、可扩展性与友好体验：\n",
    "\n",
    "* **长对话依旧连贯：** 在旧消息被截断时保留核心意图与决策。\n",
    "* **记忆效率提升：** 可将浓缩结果持久化，便于数据库存储或多设备同步。\n",
    "* **支持反思与规划：** 摘要可用于生成周报、进度追踪等结构化内容。\n",
    "* **增强用户体验：** 用户会感到被“记住”——即使隔日再来，AI 仍能复述关键目标。\n",
    "* **构建层级记忆：** 摘要还能再总结，形成“记忆树”或“滚动日志”，类似 AutoGPT、BabyAGI 的做法。\n",
    "* **降低幻觉风险：** 当模型掌握浓缩后的真实信息（如“用户周五前完成报告”），就不易生成矛盾计划或遗忘约束。\n",
    "\n",
    "换言之，摘要让您的 LLM 应用从被动聊天工具升级为**主动、有记忆的助理**，这是迈向真正智能代理的关键一步。\n",
    "\n",
    "\n",
    "## 目前成果\n",
    "\n",
    "您已经打造出这样一位 AI 助手：\n",
    "\n",
    "* 实时流式输出\n",
    "* 维护对话历史\n",
    "* 对关键决策生成摘要并记忆\n",
    "* 能在现实的词元预算内运行\n",
    "\n",
    "它不再只是聊天机器人，而是一位具备记忆、聚焦力与使命感的**认知伙伴**。\n",
    "\n",
    "更重要的是：整个过程中**无需重新训练模型**。这些能力全部来自**精心设计的提示语、上下文管理与模块化架构**——也是现代 LLM 应用的核心方法。\n",
    "\n",
    "下一节，我们将探讨如何通过系统提示塑造 **TaskFriend 的个性与行为**，无需一行额外代码。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08894b06-f9df-41d9-a06b-ae6cb82fc20b",
   "metadata": {},
   "source": [
    "# 塑造 AI 行为：系统提示\n",
    "\n",
    "---\n",
    "\n",
    "迄今我们专注于构建多轮对话应用的“外壳”。现在要回答另一个问题：**TaskFriend 究竟是谁？**\n",
    "\n",
    "答案藏在 **系统提示（system prompt）** 中——这份“隐形说明书”决定了 AI 的人格、语气、知识范围与约束。\n",
    "\n",
    "与传统软件依赖硬编码不同，LLM 的行为主要由语言塑造。一款 LLM 应用中最具威力的那行“代码”，往往就是开头这句：\n",
    "\n",
    "```python\n",
    "{\"role\": \"system\", \"content\": \"You are TaskFriend, a helpful AI assistant...\"}\n",
    "```\n",
    "\n",
    "## TaskBuckaroo：牛仔风体验\n",
    "\n",
    "让我们看看系统提示对应用的影响。给 **TaskFriend** 套上狂野西部的外衣，摇身一变成为 **TaskBuckaroo**——一位热情的牛仔效率专家。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75efa3c-db9c-4516-b52f-8c8b0f8ee11a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input our cowboy-themed system prompt\n",
    "system_prompt = ( \"\"\"\n",
    "    Howdy, partner! Yeehaw! You're TaskBuckaroo, a Wild West productivity outlaw.\n",
    "    Talk like a cowboy. Use phrases like 'reckon', 'y'all', and 'howdy'.\n",
    "    Be helpful, but make it sound like you're sippin’ coffee by the campfire.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chat.chat_interface(\n",
    "    full_conversation=[],\n",
    "    client=client,\n",
    "    call_llm_fn=wrapped_llm,\n",
    "    system_prompt_override=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f7056-251a-48ef-99a3-3a688d41ba9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "看起来相当有趣！但它**是否真的有帮助**？\n",
    "\n",
    "想象在真实应用中，用户咨询职业规划，AI 却以海盗口吻回应，或切换成莎士比亚式英语，甚至为了“角色扮演”而拒答。\n",
    "\n",
    "这就是 **提示漂移（prompt drift）**：AI 的个性凌驾于任务目标。\n",
    "\n",
    "```\n",
    "💥 模型本身没有坏。\n",
    "💥 模型也并非出现故障。\n",
    "❌ 是系统提示没有设好边界。\n",
    "```\n",
    "\n",
    "## “发挥创意”类提示的隐患\n",
    "\n",
    "诸如：\n",
    "\n",
    "* “保持有趣、有参与感！”\n",
    "* “像超级英雄一样说话！”\n",
    "* “多用表情或俚语！”\n",
    "\n",
    "……都会让行为不可控，因为它们：\n",
    "\n",
    "* 把风格置于内容之上\n",
    "* 缺乏不适场景下的防护\n",
    "* 让 AI 忘记核心任务\n",
    "\n",
    "一旦 AI 开始角色扮演，就很难再把它拉回正轨。\n",
    "\n",
    "## 解决方案：专业且结构化的系统提示\n",
    "\n",
    "让我们重新设计提示，使之简洁、稳健且易管控。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191652c-3d2b-4ba0-b64e-b335b8a5b480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input our professional, structured system prompt\n",
    "system_prompt = ( \"\"\"\n",
    "    You are TaskFriend, a professional AI assistant for productivity and work-life balance.\n",
    "\n",
    "    # Rules:\n",
    "    - Always be helpful, clear, and concise.\n",
    "    - Use neutral, professional language.\n",
    "    - Be productivity focused, help\n",
    "    - NEVER adopt accents, personas, or roleplay.\n",
    "    - If asked to roleplay, politely decline.\n",
    "    - Output structured advice when helpful (e.g., bullet points).\n",
    "    - Prioritize clarity over creativity.\n",
    "    - If user asks for harmful advice, steer them towards positive directions.\n",
    "\n",
    "    You do NOT change your tone based on user input.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chat.chat_interface(\n",
    "    full_conversation=[],\n",
    "    client=client,\n",
    "    call_llm_fn=wrapped_llm,\n",
    "    system_prompt_override=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c62db-1d49-4066-964e-c46bd8c8742c",
   "metadata": {},
   "source": [
    "## 任务关键型行为：从设计之初就锁定\n",
    "\n",
    "### 优秀系统提示的剖析\n",
    "\n",
    "系统提示就是 AI 的岗位说明书。\n",
    "若缺少清晰指令，模型就会“自由发挥”——往往弊大于利。\n",
    "\n",
    "您不会在招聘助理时只说：\n",
    "\n",
    "> “跟团队保持好氛围就行。”\n",
    "\n",
    "而是会告诉 TA：\n",
    "\n",
    "> “你需要管理日程、草拟邮件、安排会议。保持专业、准时、准确。”\n",
    "\n",
    "LLM 亦然。出色的系统提示不只是一个句子，而是一份**行为契约**。我们建议至少包含以下四个要素：\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    SP[System Prompt]\n",
    "    SP --> R[Role<br><em>You are TaskFriend...</em>]\n",
    "    SP --> G[Goal<br><em>Help users plan and reflect...</em>]\n",
    "    SP --> TF[Tone & Format<br><em>Use neutral language, bullet points...</em>]\n",
    "    SP --> GR[Guardrails<br><em>No roleplay, no slang...</em>]\n",
    "\n",
    "    style SP fill:#2196F\n",
    "    style R fill:#bbdefb\n",
    "    style G fill:#bbdefb\n",
    "    style TF fill:#bbdefb\n",
    "    style GR fill:#bbdefb\n",
    "```\n",
    "\n",
    "\n",
    "| 要素 | 作用 | 示例 |\n",
    "|-----------------|----------------------------------|-------------------------------------------------------------------------|\n",
    "| Role | 明确 AI 身份 | “你是 TaskFriend，一名效率助手。” |\n",
    "| Goal | 明确目标 | “帮助用户规划、排定优先级并复盘。” |\n",
    "| Tone & Format | 规范表达方式 | “使用清晰、正式的语言；分步骤时用要点列举。” |\n",
    "| Guardrails | 设定禁区 | “禁止角色扮演，禁止使用俚语或口音。” |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fdd8a-513b-40ee-ad16-673949df85dd",
   "metadata": {},
   "source": [
    "# 接下来做什么？\n",
    "\n",
    "## 小测验\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\"><b>1. 以下哪一点是 RAG 能帮助解决的单体 LLM 限制？</b></summary>\n",
    "\n",
    "<ul>\n",
    "    <li>A) 在旧消息被截断时保留关键决策与约束  </li>\n",
    "    <li>B) 提升词元生成速度  </li>\n",
    "    <li>C) 让模型可动态切换人设  </li>\n",
    "    <li>D) 减少 API 调用次数</li>\n",
    "</ul>\n",
    "\n",
    "**查看答案 →**\n",
    "</details>\n",
    "\n",
    "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
    "\n",
    "✅ **正确答案：** A) 在旧消息被截断时保留关键决策与约束\n",
    "📝 **解析：**\n",
    "* 摘要能在上下文窗口耗尽、早期消息被丢弃时保留语义信息（如目标、决策），在不依赖无限上下文的情况下维持连贯性。\n",
    "\n",
    "</div>\n",
    "\n",
    "## 关键收获\n",
    "\n",
    "* **上下文窗口：**\n",
    "    * 定义了 LLM 在一次请求中可处理的信息上限（以词元计）。\n",
    "    * 决定了能够包含多少对话历史、文档或数据。\n",
    "    * 具有有限性——即使 128K 词元也会被占满。\n",
    "    * 需要通过截断、摘要或索引等策略管理，避免关键内容丢失。\n",
    "    * 影响性能与正确性：一旦超限，就会发生静默数据遗失。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **摘要：**\n",
    "    * 是处理长对话、提升上下文利用率的有力工具。\n",
    "    * 保留语义要点的同时大幅降低词元消耗。\n",
    "    * 支撑无需无限上下文的持久记忆。\n",
    "    * 可分层使用（如每日摘要 → 每周回顾）形成可扩展的记忆架构。\n",
    "    * 关注可执行信息：目标、决策、约束与任务，而非泛泛话题。\n",
    "\n",
    "<br>\n",
    "\n",
    "* **系统提示：**\n",
    "    * 是 AI 的“岗位说明书”与行为契约。\n",
    "    * 明确身份、目标、语气、格式与安全边界，而不仅是塑造个性。\n",
    "    * 是传统软件硬编码逻辑的替代方案，实质上构成系统规则。\n",
    "    * 必须精准且结构化，以防提示漂移（如无意角色扮演）。\n",
    "    * 确保在多轮对话中实现一致、安全且任务导向的表现。\n",
    "    * 能像代码一样进行版本管理、测试与迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8ece8-8a1a-4e84-80c4-06b8497dcfe8",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskfriend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
